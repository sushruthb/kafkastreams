[0m[[0minfo[0m] [0mRunning structured.Simple [0m
[0m[[0minfo[0m] [0mStarting Kafka server[0m
[0m[[31merror[0m] [0mlog4j:WARN No appenders could be found for logger (utils.KafkaServer).[0m
[0m[[31merror[0m] [0mlog4j:WARN Please initialize the log4j system properly.[0m
[0m[[31merror[0m] [0mlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.[0m
[0m[[0minfo[0m] [0mCreated topic "foo".[0m
[0m[[0minfo[0m] [0m*** Publishing messages[0m
[0m[[0minfo[0m] [0m*** Starting to stream[0m
[0m[[31merror[0m] [0mUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m[[31merror[0m] [0m18/03/22 14:31:43 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 10.0.2.15 instead (on interface enp0s3)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m[[31merror[0m] [0m18/03/22 14:31:43 INFO SparkContext: Running Spark version 2.3.0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m[[31merror[0m] [0m18/03/22 14:31:44 INFO SparkContext: Submitted application: Structured_Simple[0m
[0m[[31merror[0m] [0m18/03/22 14:31:44 INFO SecurityManager: Changing view acls to: hdfs[0m
[0m[[31merror[0m] [0m18/03/22 14:31:44 INFO SecurityManager: Changing modify acls to: hdfs[0m
[0m[[31merror[0m] [0m18/03/22 14:31:44 INFO SecurityManager: Changing view acls groups to: [0m
[0m[[31merror[0m] [0m18/03/22 14:31:44 INFO SecurityManager: Changing modify acls groups to: [0m
[0m[[31merror[0m] [0m18/03/22 14:31:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hdfs); groups with view permissions: Set(); users  with modify permissions: Set(hdfs); groups with modify permissions: Set()[0m
[0m[[31merror[0m] [0m18/03/22 14:31:45 INFO Utils: Successfully started service 'sparkDriver' on port 39680.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:45 INFO SparkEnv: Registering MapOutputTracker[0m
[0m[[31merror[0m] [0m18/03/22 14:31:45 INFO SparkEnv: Registering BlockManagerMaster[0m
[0m[[31merror[0m] [0m18/03/22 14:31:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information[0m
[0m[[31merror[0m] [0m18/03/22 14:31:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up[0m
[0m[[31merror[0m] [0m18/03/22 14:31:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9fdf1470-90ac-47c1-9cb4-49351a4a6d1f[0m
[0m[[31merror[0m] [0m18/03/22 14:31:45 INFO MemoryStore: MemoryStore started with capacity 954.5 MB[0m
[0m[[31merror[0m] [0m18/03/22 14:31:45 INFO SparkEnv: Registering OutputCommitCoordinator[0m
[0m[[31merror[0m] [0m18/03/22 14:31:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4040[0m
[0m[[31merror[0m] [0m18/03/22 14:31:46 INFO Executor: Starting executor ID driver on host localhost[0m
[0m[[31merror[0m] [0m18/03/22 14:31:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38237.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:46 INFO NettyBlockTransferService: Server created on 10.0.2.15:38237[0m
[0m[[31merror[0m] [0m18/03/22 14:31:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy[0m
[0m[[31merror[0m] [0m18/03/22 14:31:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 38237, None)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:46 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:38237 with 954.5 MB RAM, BlockManagerId(driver, 10.0.2.15, 38237, None)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 38237, None)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 38237, None)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:47 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/hdfs/workspace/kafkastreaming/spark-warehouse').[0m
[0m[[31merror[0m] [0m18/03/22 14:31:47 INFO SharedState: Warehouse path is 'file:/home/hdfs/workspace/kafkastreaming/spark-warehouse'.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:48 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint[0m
[0m[[31merror[0m] [0m18/03/22 14:31:48 INFO ConsumerConfig: ConsumerConfig values: [0m
[0m[[31merror[0m] [0m	metric.reporters = [][0m
[0m[[31merror[0m] [0m	metadata.max.age.ms = 300000[0m
[0m[[31merror[0m] [0m	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor][0m
[0m[[31merror[0m] [0m	reconnect.backoff.ms = 50[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.window.factor = 0.8[0m
[0m[[31merror[0m] [0m	max.partition.fetch.bytes = 1048576[0m
[0m[[31merror[0m] [0m	bootstrap.servers = [localhost:18902][0m
[0m[[31merror[0m] [0m	ssl.keystore.type = JKS[0m
[0m[[31merror[0m] [0m	enable.auto.commit = false[0m
[0m[[31merror[0m] [0m	sasl.mechanism = GSSAPI[0m
[0m[[31merror[0m] [0m	interceptor.classes = null[0m
[0m[[31merror[0m] [0m	exclude.internal.topics = true[0m
[0m[[31merror[0m] [0m	ssl.truststore.password = null[0m
[0m[[31merror[0m] [0m	client.id = [0m
[0m[[31merror[0m] [0m	ssl.endpoint.identification.algorithm = null[0m
[0m[[31merror[0m] [0m	max.poll.records = 1[0m
[0m[[31merror[0m] [0m	check.crcs = true[0m
[0m[[31merror[0m] [0m	request.timeout.ms = 40000[0m
[0m[[31merror[0m] [0m	heartbeat.interval.ms = 3000[0m
[0m[[31merror[0m] [0m	auto.commit.interval.ms = 5000[0m
[0m[[31merror[0m] [0m	receive.buffer.bytes = 65536[0m
[0m[[31merror[0m] [0m	ssl.truststore.type = JKS[0m
[0m[[31merror[0m] [0m	ssl.truststore.location = null[0m
[0m[[31merror[0m] [0m	ssl.keystore.password = null[0m
[0m[[31merror[0m] [0m	fetch.min.bytes = 1[0m
[0m[[31merror[0m] [0m	send.buffer.bytes = 131072[0m
[0m[[31merror[0m] [0m	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	group.id = spark-kafka-source-00aec2d2-ef2b-49fd-af13-e4393968d9a0--644899932-driver-0[0m
[0m[[31merror[0m] [0m	retry.backoff.ms = 100[0m
[0m[[31merror[0m] [0m	sasl.kerberos.kinit.cmd = /usr/bin/kinit[0m
[0m[[31merror[0m] [0m	sasl.kerberos.service.name = null[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.jitter = 0.05[0m
[0m[[31merror[0m] [0m	ssl.trustmanager.algorithm = PKIX[0m
[0m[[31merror[0m] [0m	ssl.key.password = null[0m
[0m[[31merror[0m] [0m	fetch.max.wait.ms = 500[0m
[0m[[31merror[0m] [0m	sasl.kerberos.min.time.before.relogin = 60000[0m
[0m[[31merror[0m] [0m	connections.max.idle.ms = 540000[0m
[0m[[31merror[0m] [0m	session.timeout.ms = 30000[0m
[0m[[31merror[0m] [0m	metrics.num.samples = 2[0m
[0m[[31merror[0m] [0m	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	ssl.protocol = TLS[0m
[0m[[31merror[0m] [0m	ssl.provider = null[0m
[0m[[31merror[0m] [0m	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1][0m
[0m[[31merror[0m] [0m	ssl.keystore.location = null[0m
[0m[[31merror[0m] [0m	ssl.cipher.suites = null[0m
[0m[[31merror[0m] [0m	security.protocol = PLAINTEXT[0m
[0m[[31merror[0m] [0m	ssl.keymanager.algorithm = SunX509[0m
[0m[[31merror[0m] [0m	metrics.sample.window.ms = 30000[0m
[0m[[31merror[0m] [0m	auto.offset.reset = earliest[0m
[0m[[31merror[0m] [0m[0m
[0m[[31merror[0m] [0m18/03/22 14:31:48 INFO ConsumerConfig: ConsumerConfig values: [0m
[0m[[31merror[0m] [0m	metric.reporters = [][0m
[0m[[31merror[0m] [0m	metadata.max.age.ms = 300000[0m
[0m[[31merror[0m] [0m	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor][0m
[0m[[31merror[0m] [0m	reconnect.backoff.ms = 50[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.window.factor = 0.8[0m
[0m[[31merror[0m] [0m	max.partition.fetch.bytes = 1048576[0m
[0m[[31merror[0m] [0m	bootstrap.servers = [localhost:18902][0m
[0m[[31merror[0m] [0m	ssl.keystore.type = JKS[0m
[0m[[31merror[0m] [0m	enable.auto.commit = false[0m
[0m[[31merror[0m] [0m	sasl.mechanism = GSSAPI[0m
[0m[[31merror[0m] [0m	interceptor.classes = null[0m
[0m[[31merror[0m] [0m	exclude.internal.topics = true[0m
[0m[[31merror[0m] [0m	ssl.truststore.password = null[0m
[0m[[31merror[0m] [0m	client.id = consumer-1[0m
[0m[[31merror[0m] [0m	ssl.endpoint.identification.algorithm = null[0m
[0m[[31merror[0m] [0m	max.poll.records = 1[0m
[0m[[31merror[0m] [0m	check.crcs = true[0m
[0m[[31merror[0m] [0m	request.timeout.ms = 40000[0m
[0m[[31merror[0m] [0m	heartbeat.interval.ms = 3000[0m
[0m[[31merror[0m] [0m	auto.commit.interval.ms = 5000[0m
[0m[[31merror[0m] [0m	receive.buffer.bytes = 65536[0m
[0m[[31merror[0m] [0m	ssl.truststore.type = JKS[0m
[0m[[31merror[0m] [0m	ssl.truststore.location = null[0m
[0m[[31merror[0m] [0m	ssl.keystore.password = null[0m
[0m[[31merror[0m] [0m	fetch.min.bytes = 1[0m
[0m[[31merror[0m] [0m	send.buffer.bytes = 131072[0m
[0m[[31merror[0m] [0m	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	group.id = spark-kafka-source-00aec2d2-ef2b-49fd-af13-e4393968d9a0--644899932-driver-0[0m
[0m[[31merror[0m] [0m	retry.backoff.ms = 100[0m
[0m[[31merror[0m] [0m	sasl.kerberos.kinit.cmd = /usr/bin/kinit[0m
[0m[[31merror[0m] [0m	sasl.kerberos.service.name = null[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.jitter = 0.05[0m
[0m[[31merror[0m] [0m	ssl.trustmanager.algorithm = PKIX[0m
[0m[[31merror[0m] [0m	ssl.key.password = null[0m
[0m[[31merror[0m] [0m	fetch.max.wait.ms = 500[0m
[0m[[31merror[0m] [0m	sasl.kerberos.min.time.before.relogin = 60000[0m
[0m[[31merror[0m] [0m	connections.max.idle.ms = 540000[0m
[0m[[31merror[0m] [0m	session.timeout.ms = 30000[0m
[0m[[31merror[0m] [0m	metrics.num.samples = 2[0m
[0m[[31merror[0m] [0m	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	ssl.protocol = TLS[0m
[0m[[31merror[0m] [0m	ssl.provider = null[0m
[0m[[31merror[0m] [0m	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1][0m
[0m[[31merror[0m] [0m	ssl.keystore.location = null[0m
[0m[[31merror[0m] [0m	ssl.cipher.suites = null[0m
[0m[[31merror[0m] [0m	security.protocol = PLAINTEXT[0m
[0m[[31merror[0m] [0m	ssl.keymanager.algorithm = SunX509[0m
[0m[[31merror[0m] [0m	metrics.sample.window.ms = 30000[0m
[0m[[31merror[0m] [0m	auto.offset.reset = earliest[0m
[0m[[31merror[0m] [0m[0m
[0m[[31merror[0m] [0m18/03/22 14:31:48 INFO AppInfoParser: Kafka version : 0.10.0.1[0m
[0m[[31merror[0m] [0m18/03/22 14:31:48 INFO AppInfoParser: Kafka commitId : a7a17cdec9eaa6c5[0m
[0m[[31merror[0m] [0m18/03/22 14:31:50 INFO MicroBatchExecution: Starting [id = ffaedce5-66f3-429a-876a-bdbac34152a6, runId = 18f53bd6-d6a8-4f2a-97f6-cf6c0cb17ac1]. Use file:///tmp/temporary-b940620c-4d38-444d-b100-c801731d67d8 to store the query checkpoint.[0m
[0m[[0minfo[0m] [0m*** done setting up streaming[0m
[0m[[31merror[0m] [0m18/03/22 14:31:50 INFO ConsumerConfig: ConsumerConfig values: [0m
[0m[[31merror[0m] [0m	metric.reporters = [][0m
[0m[[31merror[0m] [0m	metadata.max.age.ms = 300000[0m
[0m[[31merror[0m] [0m	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor][0m
[0m[[31merror[0m] [0m	reconnect.backoff.ms = 50[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.window.factor = 0.8[0m
[0m[[31merror[0m] [0m	max.partition.fetch.bytes = 1048576[0m
[0m[[31merror[0m] [0m	bootstrap.servers = [localhost:18902][0m
[0m[[31merror[0m] [0m	ssl.keystore.type = JKS[0m
[0m[[31merror[0m] [0m	enable.auto.commit = false[0m
[0m[[31merror[0m] [0m	sasl.mechanism = GSSAPI[0m
[0m[[31merror[0m] [0m	interceptor.classes = null[0m
[0m[[31merror[0m] [0m	exclude.internal.topics = true[0m
[0m[[31merror[0m] [0m	ssl.truststore.password = null[0m
[0m[[31merror[0m] [0m	client.id = [0m
[0m[[31merror[0m] [0m	ssl.endpoint.identification.algorithm = null[0m
[0m[[31merror[0m] [0m	max.poll.records = 1[0m
[0m[[31merror[0m] [0m	check.crcs = true[0m
[0m[[31merror[0m] [0m	request.timeout.ms = 40000[0m
[0m[[31merror[0m] [0m	heartbeat.interval.ms = 3000[0m
[0m[[31merror[0m] [0m	auto.commit.interval.ms = 5000[0m
[0m[[31merror[0m] [0m	receive.buffer.bytes = 65536[0m
[0m[[31merror[0m] [0m	ssl.truststore.type = JKS[0m
[0m[[31merror[0m] [0m	ssl.truststore.location = null[0m
[0m[[31merror[0m] [0m	ssl.keystore.password = null[0m
[0m[[31merror[0m] [0m	fetch.min.bytes = 1[0m
[0m[[31merror[0m] [0m	send.buffer.bytes = 131072[0m
[0m[[31merror[0m] [0m	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	group.id = spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-driver-0[0m
[0m[[31merror[0m] [0m	retry.backoff.ms = 100[0m
[0m[[31merror[0m] [0m	sasl.kerberos.kinit.cmd = /usr/bin/kinit[0m
[0m[[31merror[0m] [0m	sasl.kerberos.service.name = null[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.jitter = 0.05[0m
[0m[[31merror[0m] [0m	ssl.trustmanager.algorithm = PKIX[0m
[0m[[31merror[0m] [0m	ssl.key.password = null[0m
[0m[[31merror[0m] [0m	fetch.max.wait.ms = 500[0m
[0m[[31merror[0m] [0m	sasl.kerberos.min.time.before.relogin = 60000[0m
[0m[[31merror[0m] [0m	connections.max.idle.ms = 540000[0m
[0m[[31merror[0m] [0m	session.timeout.ms = 30000[0m
[0m[[31merror[0m] [0m	metrics.num.samples = 2[0m
[0m[[31merror[0m] [0m	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	ssl.protocol = TLS[0m
[0m[[31merror[0m] [0m	ssl.provider = null[0m
[0m[[31merror[0m] [0m	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1][0m
[0m[[31merror[0m] [0m	ssl.keystore.location = null[0m
[0m[[31merror[0m] [0m	ssl.cipher.suites = null[0m
[0m[[31merror[0m] [0m	security.protocol = PLAINTEXT[0m
[0m[[31merror[0m] [0m	ssl.keymanager.algorithm = SunX509[0m
[0m[[31merror[0m] [0m	metrics.sample.window.ms = 30000[0m
[0m[[31merror[0m] [0m	auto.offset.reset = earliest[0m
[0m[[31merror[0m] [0m[0m
[0m[[31merror[0m] [0m18/03/22 14:31:50 INFO ConsumerConfig: ConsumerConfig values: [0m
[0m[[31merror[0m] [0m	metric.reporters = [][0m
[0m[[31merror[0m] [0m	metadata.max.age.ms = 300000[0m
[0m[[31merror[0m] [0m	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor][0m
[0m[[31merror[0m] [0m	reconnect.backoff.ms = 50[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.window.factor = 0.8[0m
[0m[[31merror[0m] [0m	max.partition.fetch.bytes = 1048576[0m
[0m[[31merror[0m] [0m	bootstrap.servers = [localhost:18902][0m
[0m[[31merror[0m] [0m	ssl.keystore.type = JKS[0m
[0m[[31merror[0m] [0m	enable.auto.commit = false[0m
[0m[[31merror[0m] [0m	sasl.mechanism = GSSAPI[0m
[0m[[31merror[0m] [0m	interceptor.classes = null[0m
[0m[[31merror[0m] [0m	exclude.internal.topics = true[0m
[0m[[31merror[0m] [0m	ssl.truststore.password = null[0m
[0m[[31merror[0m] [0m	client.id = consumer-2[0m
[0m[[31merror[0m] [0m	ssl.endpoint.identification.algorithm = null[0m
[0m[[31merror[0m] [0m	max.poll.records = 1[0m
[0m[[31merror[0m] [0m	check.crcs = true[0m
[0m[[31merror[0m] [0m	request.timeout.ms = 40000[0m
[0m[[31merror[0m] [0m	heartbeat.interval.ms = 3000[0m
[0m[[31merror[0m] [0m	auto.commit.interval.ms = 5000[0m
[0m[[31merror[0m] [0m	receive.buffer.bytes = 65536[0m
[0m[[31merror[0m] [0m	ssl.truststore.type = JKS[0m
[0m[[31merror[0m] [0m	ssl.truststore.location = null[0m
[0m[[31merror[0m] [0m	ssl.keystore.password = null[0m
[0m[[31merror[0m] [0m	fetch.min.bytes = 1[0m
[0m[[31merror[0m] [0m	send.buffer.bytes = 131072[0m
[0m[[31merror[0m] [0m	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	group.id = spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-driver-0[0m
[0m[[31merror[0m] [0m	retry.backoff.ms = 100[0m
[0m[[31merror[0m] [0m	sasl.kerberos.kinit.cmd = /usr/bin/kinit[0m
[0m[[31merror[0m] [0m	sasl.kerberos.service.name = null[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.jitter = 0.05[0m
[0m[[31merror[0m] [0m	ssl.trustmanager.algorithm = PKIX[0m
[0m[[31merror[0m] [0m	ssl.key.password = null[0m
[0m[[31merror[0m] [0m	fetch.max.wait.ms = 500[0m
[0m[[31merror[0m] [0m	sasl.kerberos.min.time.before.relogin = 60000[0m
[0m[[31merror[0m] [0m	connections.max.idle.ms = 540000[0m
[0m[[31merror[0m] [0m	session.timeout.ms = 30000[0m
[0m[[31merror[0m] [0m	metrics.num.samples = 2[0m
[0m[[31merror[0m] [0m	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	ssl.protocol = TLS[0m
[0m[[31merror[0m] [0m	ssl.provider = null[0m
[0m[[31merror[0m] [0m	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1][0m
[0m[[31merror[0m] [0m	ssl.keystore.location = null[0m
[0m[[31merror[0m] [0m	ssl.cipher.suites = null[0m
[0m[[31merror[0m] [0m	security.protocol = PLAINTEXT[0m
[0m[[31merror[0m] [0m	ssl.keymanager.algorithm = SunX509[0m
[0m[[31merror[0m] [0m	metrics.sample.window.ms = 30000[0m
[0m[[31merror[0m] [0m	auto.offset.reset = earliest[0m
[0m[[31merror[0m] [0m[0m
[0m[[31merror[0m] [0m18/03/22 14:31:50 INFO AppInfoParser: Kafka version : 0.10.0.1[0m
[0m[[31merror[0m] [0m18/03/22 14:31:50 INFO AppInfoParser: Kafka commitId : a7a17cdec9eaa6c5[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO MicroBatchExecution: Starting new streaming query.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:setData cxid:0x5b zxid:0x2b txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x5c zxid:0x2c txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO AdminUtils$: Topic creation {"version":1,"partitions":{"45":[1],"34":[1],"12":[1],"8":[1],"19":[1],"23":[1],"4":[1],"40":[1],"15":[1],"11":[1],"9":[1],"44":[1],"33":[1],"22":[1],"26":[1],"37":[1],"13":[1],"46":[1],"24":[1],"35":[1],"16":[1],"5":[1],"10":[1],"48":[1],"21":[1],"43":[1],"32":[1],"49":[1],"6":[1],"36":[1],"1":[1],"39":[1],"17":[1],"25":[1],"14":[1],"47":[1],"31":[1],"42":[1],"0":[1],"20":[1],"27":[1],"2":[1],"38":[1],"18":[1],"30":[1],"7":[1],"29":[1],"41":[1],"3":[1],"28":[1]}}[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO KafkaApis: [KafkaApi-1] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PartitionStateMachine$TopicChangeListener: [TopicChangeListener on Controller 1]: New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map([__consumer_offsets,19] -> List(1), [__consumer_offsets,30] -> List(1), [__consumer_offsets,47] -> List(1), [__consumer_offsets,29] -> List(1), [__consumer_offsets,41] -> List(1), [__consumer_offsets,39] -> List(1), [__consumer_offsets,10] -> List(1), [__consumer_offsets,17] -> List(1), [__consumer_offsets,14] -> List(1), [__consumer_offsets,40] -> List(1), [__consumer_offsets,18] -> List(1), [__consumer_offsets,26] -> List(1), [__consumer_offsets,0] -> List(1), [__consumer_offsets,24] -> List(1), [__consumer_offsets,33] -> List(1), [__consumer_offsets,20] -> List(1), [__consumer_offsets,21] -> List(1), [__consumer_offsets,3] -> List(1), [__consumer_offsets,5] -> List(1), [__consumer_offsets,22] -> List(1), [__consumer_offsets,12] -> List(1), [__consumer_offsets,8] -> List(1), [__consumer_offsets,23] -> List(1), [__consumer_offsets,15] -> List(1), [__consumer_offsets,48] -> List(1), [__consumer_offsets,11] -> List(1), [__consumer_offsets,13] -> List(1), [__consumer_offsets,49] -> List(1), [__consumer_offsets,6] -> List(1), [__consumer_offsets,28] -> List(1), [__consumer_offsets,4] -> List(1), [__consumer_offsets,37] -> List(1), [__consumer_offsets,31] -> List(1), [__consumer_offsets,44] -> List(1), [__consumer_offsets,42] -> List(1), [__consumer_offsets,34] -> List(1), [__consumer_offsets,46] -> List(1), [__consumer_offsets,25] -> List(1), [__consumer_offsets,45] -> List(1), [__consumer_offsets,27] -> List(1), [__consumer_offsets,32] -> List(1), [__consumer_offsets,43] -> List(1), [__consumer_offsets,36] -> List(1), [__consumer_offsets,35] -> List(1), [__consumer_offsets,7] -> List(1), [__consumer_offsets,9] -> List(1), [__consumer_offsets,38] -> List(1), [__consumer_offsets,1] -> List(1), [__consumer_offsets,16] -> List(1), [__consumer_offsets,2] -> List(1))][0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO KafkaController: [Controller 1]: New topic creation callback for [__consumer_offsets,32],[__consumer_offsets,16],[__consumer_offsets,49],[__consumer_offsets,44],[__consumer_offsets,28],[__consumer_offsets,17],[__consumer_offsets,23],[__consumer_offsets,7],[__consumer_offsets,4],[__consumer_offsets,29],[__consumer_offsets,35],[__consumer_offsets,3],[__consumer_offsets,24],[__consumer_offsets,41],[__consumer_offsets,0],[__consumer_offsets,38],[__consumer_offsets,13],[__consumer_offsets,8],[__consumer_offsets,5],[__consumer_offsets,39],[__consumer_offsets,36],[__consumer_offsets,40],[__consumer_offsets,45],[__consumer_offsets,15],[__consumer_offsets,33],[__consumer_offsets,37],[__consumer_offsets,21],[__consumer_offsets,6],[__consumer_offsets,11],[__consumer_offsets,20],[__consumer_offsets,47],[__consumer_offsets,2],[__consumer_offsets,27],[__consumer_offsets,34],[__consumer_offsets,9],[__consumer_offsets,22],[__consumer_offsets,42],[__consumer_offsets,14],[__consumer_offsets,25],[__consumer_offsets,10],[__consumer_offsets,48],[__consumer_offsets,31],[__consumer_offsets,18],[__consumer_offsets,19],[__consumer_offsets,12],[__consumer_offsets,46],[__consumer_offsets,43],[__consumer_offsets,1],[__consumer_offsets,26],[__consumer_offsets,30][0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO KafkaController: [Controller 1]: New partition creation callback for [__consumer_offsets,32],[__consumer_offsets,16],[__consumer_offsets,49],[__consumer_offsets,44],[__consumer_offsets,28],[__consumer_offsets,17],[__consumer_offsets,23],[__consumer_offsets,7],[__consumer_offsets,4],[__consumer_offsets,29],[__consumer_offsets,35],[__consumer_offsets,3],[__consumer_offsets,24],[__consumer_offsets,41],[__consumer_offsets,0],[__consumer_offsets,38],[__consumer_offsets,13],[__consumer_offsets,8],[__consumer_offsets,5],[__consumer_offsets,39],[__consumer_offsets,36],[__consumer_offsets,40],[__consumer_offsets,45],[__consumer_offsets,15],[__consumer_offsets,33],[__consumer_offsets,37],[__consumer_offsets,21],[__consumer_offsets,6],[__consumer_offsets,11],[__consumer_offsets,20],[__consumer_offsets,47],[__consumer_offsets,2],[__consumer_offsets,27],[__consumer_offsets,34],[__consumer_offsets,9],[__consumer_offsets,22],[__consumer_offsets,42],[__consumer_offsets,14],[__consumer_offsets,25],[__consumer_offsets,10],[__consumer_offsets,48],[__consumer_offsets,31],[__consumer_offsets,18],[__consumer_offsets,19],[__consumer_offsets,12],[__consumer_offsets,46],[__consumer_offsets,43],[__consumer_offsets,1],[__consumer_offsets,26],[__consumer_offsets,30][0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PartitionStateMachine: [Partition state machine on Controller 1]: Invoking state change to NewPartition for partitions [__consumer_offsets,32],[__consumer_offsets,16],[__consumer_offsets,49],[__consumer_offsets,44],[__consumer_offsets,28],[__consumer_offsets,17],[__consumer_offsets,23],[__consumer_offsets,7],[__consumer_offsets,4],[__consumer_offsets,29],[__consumer_offsets,35],[__consumer_offsets,3],[__consumer_offsets,24],[__consumer_offsets,41],[__consumer_offsets,0],[__consumer_offsets,38],[__consumer_offsets,13],[__consumer_offsets,8],[__consumer_offsets,5],[__consumer_offsets,39],[__consumer_offsets,36],[__consumer_offsets,40],[__consumer_offsets,45],[__consumer_offsets,15],[__consumer_offsets,33],[__consumer_offsets,37],[__consumer_offsets,21],[__consumer_offsets,6],[__consumer_offsets,11],[__consumer_offsets,20],[__consumer_offsets,47],[__consumer_offsets,2],[__consumer_offsets,27],[__consumer_offsets,34],[__consumer_offsets,9],[__consumer_offsets,22],[__consumer_offsets,42],[__consumer_offsets,14],[__consumer_offsets,25],[__consumer_offsets,10],[__consumer_offsets,48],[__consumer_offsets,31],[__consumer_offsets,18],[__consumer_offsets,19],[__consumer_offsets,12],[__consumer_offsets,46],[__consumer_offsets,43],[__consumer_offsets,1],[__consumer_offsets,26],[__consumer_offsets,30][0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO ReplicaStateMachine: [Replica state machine on controller 1]: Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=28,Replica=1],[Topic=__consumer_offsets,Partition=48,Replica=1],[Topic=__consumer_offsets,Partition=5,Replica=1],[Topic=__consumer_offsets,Partition=21,Replica=1],[Topic=__consumer_offsets,Partition=2,Replica=1],[Topic=__consumer_offsets,Partition=18,Replica=1],[Topic=__consumer_offsets,Partition=23,Replica=1],[Topic=__consumer_offsets,Partition=9,Replica=1],[Topic=__consumer_offsets,Partition=39,Replica=1],[Topic=__consumer_offsets,Partition=31,Replica=1],[Topic=__consumer_offsets,Partition=19,Replica=1],[Topic=__consumer_offsets,Partition=10,Replica=1],[Topic=__consumer_offsets,Partition=22,Replica=1],[Topic=__consumer_offsets,Partition=43,Replica=1],[Topic=__consumer_offsets,Partition=40,Replica=1],[Topic=__consumer_offsets,Partition=27,Replica=1],[Topic=__consumer_offsets,Partition=6,Replica=1],[Topic=__consumer_offsets,Partition=1,Replica=1],[Topic=__consumer_offsets,Partition=47,Replica=1],[Topic=__consumer_offsets,Partition=30,Replica=1],[Topic=__consumer_offsets,Partition=42,Replica=1],[Topic=__consumer_offsets,Partition=41,Replica=1],[Topic=__consumer_offsets,Partition=3,Replica=1],[Topic=__consumer_offsets,Partition=13,Replica=1],[Topic=__consumer_offsets,Partition=4,Replica=1],[Topic=__consumer_offsets,Partition=16,Replica=1],[Topic=__consumer_offsets,Partition=46,Replica=1],[Topic=__consumer_offsets,Partition=49,Replica=1],[Topic=__consumer_offsets,Partition=14,Replica=1],[Topic=__consumer_offsets,Partition=45,Replica=1],[Topic=__consumer_offsets,Partition=37,Replica=1],[Topic=__consumer_offsets,Partition=29,Replica=1],[Topic=__consumer_offsets,Partition=20,Replica=1],[Topic=__consumer_offsets,Partition=8,Replica=1],[Topic=__consumer_offsets,Partition=38,Replica=1],[Topic=__consumer_offsets,Partition=7,Replica=1],[Topic=__consumer_offsets,Partition=0,Replica=1],[Topic=__consumer_offsets,Partition=34,Replica=1],[Topic=__consumer_offsets,Partition=33,Replica=1],[Topic=__consumer_offsets,Partition=26,Replica=1],[Topic=__consumer_offsets,Partition=44,Replica=1],[Topic=__consumer_offsets,Partition=32,Replica=1],[Topic=__consumer_offsets,Partition=25,Replica=1],[Topic=__consumer_offsets,Partition=11,Replica=1],[Topic=__consumer_offsets,Partition=36,Replica=1],[Topic=__consumer_offsets,Partition=12,Replica=1],[Topic=__consumer_offsets,Partition=35,Replica=1],[Topic=__consumer_offsets,Partition=15,Replica=1],[Topic=__consumer_offsets,Partition=17,Replica=1],[Topic=__consumer_offsets,Partition=24,Replica=1][0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PartitionStateMachine: [Partition state machine on Controller 1]: Invoking state change to OnlinePartition for partitions [__consumer_offsets,32],[__consumer_offsets,16],[__consumer_offsets,49],[__consumer_offsets,44],[__consumer_offsets,28],[__consumer_offsets,17],[__consumer_offsets,23],[__consumer_offsets,7],[__consumer_offsets,4],[__consumer_offsets,29],[__consumer_offsets,35],[__consumer_offsets,3],[__consumer_offsets,24],[__consumer_offsets,41],[__consumer_offsets,0],[__consumer_offsets,38],[__consumer_offsets,13],[__consumer_offsets,8],[__consumer_offsets,5],[__consumer_offsets,39],[__consumer_offsets,36],[__consumer_offsets,40],[__consumer_offsets,45],[__consumer_offsets,15],[__consumer_offsets,33],[__consumer_offsets,37],[__consumer_offsets,21],[__consumer_offsets,6],[__consumer_offsets,11],[__consumer_offsets,20],[__consumer_offsets,47],[__consumer_offsets,2],[__consumer_offsets,27],[__consumer_offsets,34],[__consumer_offsets,9],[__consumer_offsets,22],[__consumer_offsets,42],[__consumer_offsets,14],[__consumer_offsets,25],[__consumer_offsets,10],[__consumer_offsets,48],[__consumer_offsets,31],[__consumer_offsets,18],[__consumer_offsets,19],[__consumer_offsets,12],[__consumer_offsets,46],[__consumer_offsets,43],[__consumer_offsets,1],[__consumer_offsets,26],[__consumer_offsets,30][0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x98 zxid:0x2f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/32 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/32[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x99 zxid:0x30 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x9d zxid:0x34 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/16 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/16[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xa3 zxid:0x37 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/49 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/49[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xa6 zxid:0x3a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/44 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/44[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xa9 zxid:0x3d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/28 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/28[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xac zxid:0x40 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/17 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/17[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xb1 zxid:0x43 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/23 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/23[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xb5 zxid:0x46 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/7 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/7[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xb8 zxid:0x49 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/4 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/4[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xbb zxid:0x4c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/29 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/29[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xbe zxid:0x4f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/35 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/35[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xc4 zxid:0x52 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/3 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/3[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xc7 zxid:0x55 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/24 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/24[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xca zxid:0x58 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/41 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/41[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xce zxid:0x5b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xd3 zxid:0x5e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/38 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/38[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xd6 zxid:0x61 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/13 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/13[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xd9 zxid:0x64 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/8 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/8[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xde zxid:0x67 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/5 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/5[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xe2 zxid:0x6a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/39 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/39[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xe5 zxid:0x6d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/36 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/36[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xe8 zxid:0x70 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/40 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/40[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xeb zxid:0x73 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/45 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/45[0m
[0m[[31merror[0m] [0m18/03/22 14:31:51 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xf1 zxid:0x76 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/15 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/15[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xf4 zxid:0x79 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/33 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/33[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xf7 zxid:0x7c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/37 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/37[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xfa zxid:0x7f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/21 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/21[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0xff zxid:0x82 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/6 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/6[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x103 zxid:0x85 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/11 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/11[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x106 zxid:0x88 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/20 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/20[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x109 zxid:0x8b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/47 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/47[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x10c zxid:0x8e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/2 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/2[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x110 zxid:0x91 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/27 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/27[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x115 zxid:0x94 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/34 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/34[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x118 zxid:0x97 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/9 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/9[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x11b zxid:0x9a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/22 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/22[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x11e zxid:0x9d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/42 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/42[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x124 zxid:0xa0 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/14 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/14[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x127 zxid:0xa3 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/25 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/25[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x12a zxid:0xa6 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/10 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/10[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x12d zxid:0xa9 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/48 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/48[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x133 zxid:0xac txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/31 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/31[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x136 zxid:0xaf txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/18 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/18[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x139 zxid:0xb2 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/19 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/19[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x13c zxid:0xb5 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/12 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/12[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x13f zxid:0xb8 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/46 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/46[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x142 zxid:0xbb txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/43 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/43[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x148 zxid:0xbe txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/1 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/1[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x14b zxid:0xc1 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/26 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/26[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x1624cf118e20000 type:create cxid:0x14e zxid:0xc4 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/30 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/30[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO ReplicaStateMachine: [Replica state machine on controller 1]: Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=28,Replica=1],[Topic=__consumer_offsets,Partition=48,Replica=1],[Topic=__consumer_offsets,Partition=5,Replica=1],[Topic=__consumer_offsets,Partition=21,Replica=1],[Topic=__consumer_offsets,Partition=2,Replica=1],[Topic=__consumer_offsets,Partition=18,Replica=1],[Topic=__consumer_offsets,Partition=23,Replica=1],[Topic=__consumer_offsets,Partition=9,Replica=1],[Topic=__consumer_offsets,Partition=39,Replica=1],[Topic=__consumer_offsets,Partition=31,Replica=1],[Topic=__consumer_offsets,Partition=19,Replica=1],[Topic=__consumer_offsets,Partition=10,Replica=1],[Topic=__consumer_offsets,Partition=22,Replica=1],[Topic=__consumer_offsets,Partition=43,Replica=1],[Topic=__consumer_offsets,Partition=40,Replica=1],[Topic=__consumer_offsets,Partition=27,Replica=1],[Topic=__consumer_offsets,Partition=6,Replica=1],[Topic=__consumer_offsets,Partition=1,Replica=1],[Topic=__consumer_offsets,Partition=47,Replica=1],[Topic=__consumer_offsets,Partition=30,Replica=1],[Topic=__consumer_offsets,Partition=42,Replica=1],[Topic=__consumer_offsets,Partition=41,Replica=1],[Topic=__consumer_offsets,Partition=3,Replica=1],[Topic=__consumer_offsets,Partition=13,Replica=1],[Topic=__consumer_offsets,Partition=4,Replica=1],[Topic=__consumer_offsets,Partition=16,Replica=1],[Topic=__consumer_offsets,Partition=46,Replica=1],[Topic=__consumer_offsets,Partition=49,Replica=1],[Topic=__consumer_offsets,Partition=14,Replica=1],[Topic=__consumer_offsets,Partition=45,Replica=1],[Topic=__consumer_offsets,Partition=37,Replica=1],[Topic=__consumer_offsets,Partition=29,Replica=1],[Topic=__consumer_offsets,Partition=20,Replica=1],[Topic=__consumer_offsets,Partition=8,Replica=1],[Topic=__consumer_offsets,Partition=38,Replica=1],[Topic=__consumer_offsets,Partition=7,Replica=1],[Topic=__consumer_offsets,Partition=0,Replica=1],[Topic=__consumer_offsets,Partition=34,Replica=1],[Topic=__consumer_offsets,Partition=33,Replica=1],[Topic=__consumer_offsets,Partition=26,Replica=1],[Topic=__consumer_offsets,Partition=44,Replica=1],[Topic=__consumer_offsets,Partition=32,Replica=1],[Topic=__consumer_offsets,Partition=25,Replica=1],[Topic=__consumer_offsets,Partition=11,Replica=1],[Topic=__consumer_offsets,Partition=36,Replica=1],[Topic=__consumer_offsets,Partition=12,Replica=1],[Topic=__consumer_offsets,Partition=35,Replica=1],[Topic=__consumer_offsets,Partition=15,Replica=1],[Topic=__consumer_offsets,Partition=17,Replica=1],[Topic=__consumer_offsets,Partition=24,Replica=1][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 1] Removed fetcher for partitions [__consumer_offsets,32],[__consumer_offsets,16],[__consumer_offsets,49],[__consumer_offsets,44],[__consumer_offsets,28],[__consumer_offsets,17],[__consumer_offsets,23],[__consumer_offsets,7],[__consumer_offsets,4],[__consumer_offsets,29],[__consumer_offsets,35],[__consumer_offsets,3],[__consumer_offsets,24],[__consumer_offsets,41],[__consumer_offsets,0],[__consumer_offsets,38],[__consumer_offsets,13],[__consumer_offsets,8],[__consumer_offsets,5],[__consumer_offsets,39],[__consumer_offsets,36],[__consumer_offsets,40],[__consumer_offsets,45],[__consumer_offsets,15],[__consumer_offsets,33],[__consumer_offsets,37],[__consumer_offsets,21],[__consumer_offsets,6],[__consumer_offsets,11],[__consumer_offsets,20],[__consumer_offsets,47],[__consumer_offsets,2],[__consumer_offsets,27],[__consumer_offsets,34],[__consumer_offsets,9],[__consumer_offsets,22],[__consumer_offsets,42],[__consumer_offsets,14],[__consumer_offsets,25],[__consumer_offsets,10],[__consumer_offsets,48],[__consumer_offsets,31],[__consumer_offsets,18],[__consumer_offsets,19],[__consumer_offsets,12],[__consumer_offsets,46],[__consumer_offsets,43],[__consumer_offsets,1],[__consumer_offsets,26],[__consumer_offsets,30][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-0 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,0] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,0] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,0][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-29 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,29] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,29] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,29][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-48 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,48] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,48] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,48][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-10 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,10] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,10] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,10][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-45 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,45] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,45] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,45][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-26 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,26] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,26] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,26][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-7 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,7] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,7] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,7][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-42 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,42] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,42] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,42][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-4 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,4] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,4] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,4][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-23 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,23] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,23] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,23][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-1 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,1] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,1] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,1][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-20 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,20] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,20] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,20][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-39 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,39] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,39] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,39][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-17 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,17] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,17] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,17][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-36 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,36] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,36] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,36][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-14 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,14] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,14] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,14][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-33 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,33] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,33] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,33][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-49 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,49] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,49] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,49][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-11 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,11] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,11] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,11][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-30 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,30] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,30] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,30][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-46 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,46] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,46] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,46][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-27 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,27] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,27] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,27][0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Log: Completed load of log __consumer_offsets-8 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO LogManager: Created log for partition [__consumer_offsets,8] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:52 INFO Partition: Partition [__consumer_offsets,8] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,8][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-24 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,24] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,24] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,24][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-43 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,43] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,43] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,43][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-5 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,5] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,5] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,5][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-21 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,21] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,21] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,21][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-2 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,2] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,2] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,2][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-40 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,40] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,40] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,40][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-37 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,37] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,37] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,37][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-18 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,18] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,18] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,18][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-34 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,34] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,34] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,34][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-15 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,15] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,15] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,15][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-12 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,12] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,12] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,12][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-31 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,31] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,31] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,31][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-9 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,9] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,9] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,9][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-47 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,47] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,47] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,47][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-19 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,19] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,19] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,19][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-28 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,28] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,28] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,28][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-38 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,38] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,38] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,38][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-35 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,35] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,35] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,35][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-44 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,44] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,44] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,44][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-6 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,6] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,6] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,6][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-25 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,25] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,25] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,25][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-16 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,16] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,16] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,16][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-22 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,22] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,22] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,22][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-41 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,41] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,41] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,41][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-32 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,32] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,32] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,32][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-3 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,3] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,3] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,3][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Log: Completed load of log __consumer_offsets-13 with log end offset 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO LogManager: Created log for partition [__consumer_offsets,13] in /tmp/SSWK/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.10.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO Partition: Partition [__consumer_offsets,13] on broker 1: No checkpointed highwatermark is found for partition [__consumer_offsets,13][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,22][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,22] in 4 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,25][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,25] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,28][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,28] in 0 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,31][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,31] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,34][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,34] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,37][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,37] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,40][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,40] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,43][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,43] in 22 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,46][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,46] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,49][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,49] in 8 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,41][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,41] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,44][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,44] in 0 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,47][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,47] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,1][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,1] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,4][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,4] in 0 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,7][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,7] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,10][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,10] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,13][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,13] in 0 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,16][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,16] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,19][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,19] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,2][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,2] in 0 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,5][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,5] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,8][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,8] in 0 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,11][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,11] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,14][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,14] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,17][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,17] in 0 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,20][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,20] in 31 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,23][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,23] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,26][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO AbstractCoordinator: Discovered coordinator localhost:18902 (id: 2147483646 rack: null) for group spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-driver-0.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO ConsumerCoordinator: Revoking previously assigned partitions [] for group spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-driver-0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO AbstractCoordinator: (Re-)joining group spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-driver-0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO AbstractCoordinator: Marking the coordinator localhost:18902 (id: 2147483646 rack: null) dead for group spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-driver-0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,26] in 27 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,29][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,29] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,32][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,32] in 0 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,35][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,35] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,38][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,38] in 0 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,0][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,0] in 0 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,3][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,3] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,6][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,6] in 0 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,9][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,9] in 0 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,12][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,12] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,15][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,15] in 0 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,18][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,18] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,21][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,21] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,24][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,24] in 0 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,27][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,27] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,30][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,30] in 0 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,33][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,33] in 40 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,36][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,36] in 0 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,39][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,39] in 8 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,42][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,42] in 0 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,45][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,45] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from [__consumer_offsets,48][0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupMetadataManager: [Group Metadata Manager on Broker 1]: Finished loading offsets from [__consumer_offsets,48] in 1 milliseconds.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO AbstractCoordinator: Discovered coordinator localhost:18902 (id: 2147483646 rack: null) for group spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-driver-0.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO AbstractCoordinator: (Re-)joining group spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-driver-0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupCoordinator: [GroupCoordinator 1]: Preparing to restabilize group spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-driver-0 with old generation 0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupCoordinator: [GroupCoordinator 1]: Stabilized group spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-driver-0 generation 1[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO GroupCoordinator: [GroupCoordinator 1]: Assignment received from leader for group spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-driver-0 for generation 1[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO AbstractCoordinator: Successfully joined group spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-driver-0 with generation 1[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO ConsumerCoordinator: Setting newly assigned partitions [foo-2, foo-3, foo-0, foo-1] for group spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-driver-0[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO KafkaSource: Initial offsets: {"foo":{"2":0,"1":0,"3":0,"0":0}}[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1521709313872,Map(spark.sql.shuffle.partitions -> 200, spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider))[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO KafkaSource: GetBatch called with start = None, end = {"foo":{"2":1,"1":2,"3":1,"0":1}}[0m
[0m[[31merror[0m] [0m18/03/22 14:31:53 INFO KafkaSource: Partitions added: Map()[0m
[0m[[31merror[0m] [0m18/03/22 14:31:54 INFO KafkaSource: GetBatch generating RDD of offset range: KafkaSourceRDDOffsetRange(foo-0,0,1,None), KafkaSourceRDDOffsetRange(foo-1,0,2,None), KafkaSourceRDDOffsetRange(foo-2,0,1,None), KafkaSourceRDDOffsetRange(foo-3,0,1,None)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:55 INFO CodeGenerator: Code generated in 354.605575 ms[0m
[0m[[31merror[0m] [0m18/03/22 14:31:55 INFO CodeGenerator: Code generated in 73.785318 ms[0m
[0m[[31merror[0m] [0m18/03/22 14:31:55 INFO WriteToDataSourceV2Exec: Start processing data source writer: org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@61b64d4b. The input RDD has 4 partitions.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:55 INFO SparkContext: Starting job: start at Simple.scala:49[0m
[0m[[31merror[0m] [0m18/03/22 14:31:55 INFO DAGScheduler: Got job 0 (start at Simple.scala:49) with 4 output partitions[0m
[0m[[31merror[0m] [0m18/03/22 14:31:55 INFO DAGScheduler: Final stage: ResultStage 0 (start at Simple.scala:49)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:55 INFO DAGScheduler: Parents of final stage: List()[0m
[0m[[31merror[0m] [0m18/03/22 14:31:55 INFO DAGScheduler: Missing parents: List()[0m
[0m[[31merror[0m] [0m18/03/22 14:31:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at start at Simple.scala:49), which has no missing parents[0m
[0m[[0minfo[0m] [0m*** publishing more messages[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.8 KB, free 954.5 MB)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.1 KB, free 954.5 MB)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:38237 (size: 5.1 KB, free: 954.5 MB)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1039[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at start at Simple.scala:49) (first 15 tasks are for partitions Vector(0, 1, 2, 3))[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8026 bytes)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 8026 bytes)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 8026 bytes)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 8026 bytes)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO ConsumerConfig: ConsumerConfig values: [0m
[0m[[31merror[0m] [0m	metric.reporters = [][0m
[0m[[31merror[0m] [0m	metadata.max.age.ms = 300000[0m
[0m[[31merror[0m] [0m	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor][0m
[0m[[31merror[0m] [0m	reconnect.backoff.ms = 50[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.window.factor = 0.8[0m
[0m[[31merror[0m] [0m	max.partition.fetch.bytes = 1048576[0m
[0m[[31merror[0m] [0m	bootstrap.servers = [localhost:18902][0m
[0m[[31merror[0m] [0m	ssl.keystore.type = JKS[0m
[0m[[31merror[0m] [0m	enable.auto.commit = false[0m
[0m[[31merror[0m] [0m	sasl.mechanism = GSSAPI[0m
[0m[[31merror[0m] [0m	interceptor.classes = null[0m
[0m[[31merror[0m] [0m	exclude.internal.topics = true[0m
[0m[[31merror[0m] [0m	ssl.truststore.password = null[0m
[0m[[31merror[0m] [0m	client.id = [0m
[0m[[31merror[0m] [0m	ssl.endpoint.identification.algorithm = null[0m
[0m[[31merror[0m] [0m	max.poll.records = 2147483647[0m
[0m[[31merror[0m] [0m	check.crcs = true[0m
[0m[[31merror[0m] [0m	request.timeout.ms = 40000[0m
[0m[[31merror[0m] [0m	heartbeat.interval.ms = 3000[0m
[0m[[31merror[0m] [0m	auto.commit.interval.ms = 5000[0m
[0m[[31merror[0m] [0m	receive.buffer.bytes = 65536[0m
[0m[[31merror[0m] [0m	ssl.truststore.type = JKS[0m
[0m[[31merror[0m] [0m	ssl.truststore.location = null[0m
[0m[[31merror[0m] [0m	ssl.keystore.password = null[0m
[0m[[31merror[0m] [0m	fetch.min.bytes = 1[0m
[0m[[31merror[0m] [0m	send.buffer.bytes = 131072[0m
[0m[[31merror[0m] [0m	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	group.id = spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-executor[0m
[0m[[31merror[0m] [0m	retry.backoff.ms = 100[0m
[0m[[31merror[0m] [0m	sasl.kerberos.kinit.cmd = /usr/bin/kinit[0m
[0m[[31merror[0m] [0m	sasl.kerberos.service.name = null[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.jitter = 0.05[0m
[0m[[31merror[0m] [0m	ssl.trustmanager.algorithm = PKIX[0m
[0m[[31merror[0m] [0m	ssl.key.password = null[0m
[0m[[31merror[0m] [0m	fetch.max.wait.ms = 500[0m
[0m[[31merror[0m] [0m	sasl.kerberos.min.time.before.relogin = 60000[0m
[0m[[31merror[0m] [0m	connections.max.idle.ms = 540000[0m
[0m[[31merror[0m] [0m	session.timeout.ms = 30000[0m
[0m[[31merror[0m] [0m	metrics.num.samples = 2[0m
[0m[[31merror[0m] [0m	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	ssl.protocol = TLS[0m
[0m[[31merror[0m] [0m	ssl.provider = null[0m
[0m[[31merror[0m] [0m	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1][0m
[0m[[31merror[0m] [0m	ssl.keystore.location = null[0m
[0m[[31merror[0m] [0m	ssl.cipher.suites = null[0m
[0m[[31merror[0m] [0m	security.protocol = PLAINTEXT[0m
[0m[[31merror[0m] [0m	ssl.keymanager.algorithm = SunX509[0m
[0m[[31merror[0m] [0m	metrics.sample.window.ms = 30000[0m
[0m[[31merror[0m] [0m	auto.offset.reset = none[0m
[0m[[31merror[0m] [0m[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO ConsumerConfig: ConsumerConfig values: [0m
[0m[[31merror[0m] [0m	metric.reporters = [][0m
[0m[[31merror[0m] [0m	metadata.max.age.ms = 300000[0m
[0m[[31merror[0m] [0m	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor][0m
[0m[[31merror[0m] [0m	reconnect.backoff.ms = 50[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.window.factor = 0.8[0m
[0m[[31merror[0m] [0m	max.partition.fetch.bytes = 1048576[0m
[0m[[31merror[0m] [0m	bootstrap.servers = [localhost:18902][0m
[0m[[31merror[0m] [0m	ssl.keystore.type = JKS[0m
[0m[[31merror[0m] [0m	enable.auto.commit = false[0m
[0m[[31merror[0m] [0m	sasl.mechanism = GSSAPI[0m
[0m[[31merror[0m] [0m	interceptor.classes = null[0m
[0m[[31merror[0m] [0m	exclude.internal.topics = true[0m
[0m[[31merror[0m] [0m	ssl.truststore.password = null[0m
[0m[[31merror[0m] [0m	client.id = consumer-3[0m
[0m[[31merror[0m] [0m	ssl.endpoint.identification.algorithm = null[0m
[0m[[31merror[0m] [0m	max.poll.records = 2147483647[0m
[0m[[31merror[0m] [0m	check.crcs = true[0m
[0m[[31merror[0m] [0m	request.timeout.ms = 40000[0m
[0m[[31merror[0m] [0m	heartbeat.interval.ms = 3000[0m
[0m[[31merror[0m] [0m	auto.commit.interval.ms = 5000[0m
[0m[[31merror[0m] [0m	receive.buffer.bytes = 65536[0m
[0m[[31merror[0m] [0m	ssl.truststore.type = JKS[0m
[0m[[31merror[0m] [0m	ssl.truststore.location = null[0m
[0m[[31merror[0m] [0m	ssl.keystore.password = null[0m
[0m[[31merror[0m] [0m	fetch.min.bytes = 1[0m
[0m[[31merror[0m] [0m	send.buffer.bytes = 131072[0m
[0m[[31merror[0m] [0m	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	group.id = spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-executor[0m
[0m[[31merror[0m] [0m	retry.backoff.ms = 100[0m
[0m[[31merror[0m] [0m	sasl.kerberos.kinit.cmd = /usr/bin/kinit[0m
[0m[[31merror[0m] [0m	sasl.kerberos.service.name = null[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.jitter = 0.05[0m
[0m[[31merror[0m] [0m	ssl.trustmanager.algorithm = PKIX[0m
[0m[[31merror[0m] [0m	ssl.key.password = null[0m
[0m[[31merror[0m] [0m	fetch.max.wait.ms = 500[0m
[0m[[31merror[0m] [0m	sasl.kerberos.min.time.before.relogin = 60000[0m
[0m[[31merror[0m] [0m	connections.max.idle.ms = 540000[0m
[0m[[31merror[0m] [0m	session.timeout.ms = 30000[0m
[0m[[31merror[0m] [0m	metrics.num.samples = 2[0m
[0m[[31merror[0m] [0m	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	ssl.protocol = TLS[0m
[0m[[31merror[0m] [0m	ssl.provider = null[0m
[0m[[31merror[0m] [0m	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1][0m
[0m[[31merror[0m] [0m	ssl.keystore.location = null[0m
[0m[[31merror[0m] [0m	ssl.cipher.suites = null[0m
[0m[[31merror[0m] [0m	security.protocol = PLAINTEXT[0m
[0m[[31merror[0m] [0m	ssl.keymanager.algorithm = SunX509[0m
[0m[[31merror[0m] [0m	metrics.sample.window.ms = 30000[0m
[0m[[31merror[0m] [0m	auto.offset.reset = none[0m
[0m[[31merror[0m] [0m[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO AppInfoParser: Kafka version : 0.10.0.1[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO AppInfoParser: Kafka commitId : a7a17cdec9eaa6c5[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO ConsumerConfig: ConsumerConfig values: [0m
[0m[[31merror[0m] [0m	metric.reporters = [][0m
[0m[[31merror[0m] [0m	metadata.max.age.ms = 300000[0m
[0m[[31merror[0m] [0m	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor][0m
[0m[[31merror[0m] [0m	reconnect.backoff.ms = 50[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.window.factor = 0.8[0m
[0m[[31merror[0m] [0m	max.partition.fetch.bytes = 1048576[0m
[0m[[31merror[0m] [0m	bootstrap.servers = [localhost:18902][0m
[0m[[31merror[0m] [0m	ssl.keystore.type = JKS[0m
[0m[[31merror[0m] [0m	enable.auto.commit = false[0m
[0m[[31merror[0m] [0m	sasl.mechanism = GSSAPI[0m
[0m[[31merror[0m] [0m	interceptor.classes = null[0m
[0m[[31merror[0m] [0m	exclude.internal.topics = true[0m
[0m[[31merror[0m] [0m	ssl.truststore.password = null[0m
[0m[[31merror[0m] [0m	client.id = [0m
[0m[[31merror[0m] [0m	ssl.endpoint.identification.algorithm = null[0m
[0m[[31merror[0m] [0m	max.poll.records = 2147483647[0m
[0m[[31merror[0m] [0m	check.crcs = true[0m
[0m[[31merror[0m] [0m	request.timeout.ms = 40000[0m
[0m[[31merror[0m] [0m	heartbeat.interval.ms = 3000[0m
[0m[[31merror[0m] [0m	auto.commit.interval.ms = 5000[0m
[0m[[31merror[0m] [0m	receive.buffer.bytes = 65536[0m
[0m[[31merror[0m] [0m	ssl.truststore.type = JKS[0m
[0m[[31merror[0m] [0m	ssl.truststore.location = null[0m
[0m[[31merror[0m] [0m	ssl.keystore.password = null[0m
[0m[[31merror[0m] [0m	fetch.min.bytes = 1[0m
[0m[[31merror[0m] [0m	send.buffer.bytes = 131072[0m
[0m[[31merror[0m] [0m	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	group.id = spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-executor[0m
[0m[[31merror[0m] [0m	retry.backoff.ms = 100[0m
[0m[[31merror[0m] [0m	sasl.kerberos.kinit.cmd = /usr/bin/kinit[0m
[0m[[31merror[0m] [0m	sasl.kerberos.service.name = null[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.jitter = 0.05[0m
[0m[[31merror[0m] [0m	ssl.trustmanager.algorithm = PKIX[0m
[0m[[31merror[0m] [0m	ssl.key.password = null[0m
[0m[[31merror[0m] [0m	fetch.max.wait.ms = 500[0m
[0m[[31merror[0m] [0m	sasl.kerberos.min.time.before.relogin = 60000[0m
[0m[[31merror[0m] [0m	connections.max.idle.ms = 540000[0m
[0m[[31merror[0m] [0m	session.timeout.ms = 30000[0m
[0m[[31merror[0m] [0m	metrics.num.samples = 2[0m
[0m[[31merror[0m] [0m	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	ssl.protocol = TLS[0m
[0m[[31merror[0m] [0m	ssl.provider = null[0m
[0m[[31merror[0m] [0m	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1][0m
[0m[[31merror[0m] [0m	ssl.keystore.location = null[0m
[0m[[31merror[0m] [0m	ssl.cipher.suites = null[0m
[0m[[31merror[0m] [0m	security.protocol = PLAINTEXT[0m
[0m[[31merror[0m] [0m	ssl.keymanager.algorithm = SunX509[0m
[0m[[31merror[0m] [0m	metrics.sample.window.ms = 30000[0m
[0m[[31merror[0m] [0m	auto.offset.reset = none[0m
[0m[[31merror[0m] [0m[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO ConsumerConfig: ConsumerConfig values: [0m
[0m[[31merror[0m] [0m	metric.reporters = [][0m
[0m[[31merror[0m] [0m	metadata.max.age.ms = 300000[0m
[0m[[31merror[0m] [0m	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor][0m
[0m[[31merror[0m] [0m	reconnect.backoff.ms = 50[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.window.factor = 0.8[0m
[0m[[31merror[0m] [0m	max.partition.fetch.bytes = 1048576[0m
[0m[[31merror[0m] [0m	bootstrap.servers = [localhost:18902][0m
[0m[[31merror[0m] [0m	ssl.keystore.type = JKS[0m
[0m[[31merror[0m] [0m	enable.auto.commit = false[0m
[0m[[31merror[0m] [0m	sasl.mechanism = GSSAPI[0m
[0m[[31merror[0m] [0m	interceptor.classes = null[0m
[0m[[31merror[0m] [0m	exclude.internal.topics = true[0m
[0m[[31merror[0m] [0m	ssl.truststore.password = null[0m
[0m[[31merror[0m] [0m	client.id = consumer-4[0m
[0m[[31merror[0m] [0m	ssl.endpoint.identification.algorithm = null[0m
[0m[[31merror[0m] [0m	max.poll.records = 2147483647[0m
[0m[[31merror[0m] [0m	check.crcs = true[0m
[0m[[31merror[0m] [0m	request.timeout.ms = 40000[0m
[0m[[31merror[0m] [0m	heartbeat.interval.ms = 3000[0m
[0m[[31merror[0m] [0m	auto.commit.interval.ms = 5000[0m
[0m[[31merror[0m] [0m	receive.buffer.bytes = 65536[0m
[0m[[31merror[0m] [0m	ssl.truststore.type = JKS[0m
[0m[[31merror[0m] [0m	ssl.truststore.location = null[0m
[0m[[31merror[0m] [0m	ssl.keystore.password = null[0m
[0m[[31merror[0m] [0m	fetch.min.bytes = 1[0m
[0m[[31merror[0m] [0m	send.buffer.bytes = 131072[0m
[0m[[31merror[0m] [0m	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	group.id = spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-executor[0m
[0m[[31merror[0m] [0m	retry.backoff.ms = 100[0m
[0m[[31merror[0m] [0m	sasl.kerberos.kinit.cmd = /usr/bin/kinit[0m
[0m[[31merror[0m] [0m	sasl.kerberos.service.name = null[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.jitter = 0.05[0m
[0m[[31merror[0m] [0m	ssl.trustmanager.algorithm = PKIX[0m
[0m[[31merror[0m] [0m	ssl.key.password = null[0m
[0m[[31merror[0m] [0m	fetch.max.wait.ms = 500[0m
[0m[[31merror[0m] [0m	sasl.kerberos.min.time.before.relogin = 60000[0m
[0m[[31merror[0m] [0m	connections.max.idle.ms = 540000[0m
[0m[[31merror[0m] [0m	session.timeout.ms = 30000[0m
[0m[[31merror[0m] [0m	metrics.num.samples = 2[0m
[0m[[31merror[0m] [0m	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	ssl.protocol = TLS[0m
[0m[[31merror[0m] [0m	ssl.provider = null[0m
[0m[[31merror[0m] [0m	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1][0m
[0m[[31merror[0m] [0m	ssl.keystore.location = null[0m
[0m[[31merror[0m] [0m	ssl.cipher.suites = null[0m
[0m[[31merror[0m] [0m	security.protocol = PLAINTEXT[0m
[0m[[31merror[0m] [0m	ssl.keymanager.algorithm = SunX509[0m
[0m[[31merror[0m] [0m	metrics.sample.window.ms = 30000[0m
[0m[[31merror[0m] [0m	auto.offset.reset = none[0m
[0m[[31merror[0m] [0m[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO AppInfoParser: Kafka version : 0.10.0.1[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO AppInfoParser: Kafka commitId : a7a17cdec9eaa6c5[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO ConsumerConfig: ConsumerConfig values: [0m
[0m[[31merror[0m] [0m	metric.reporters = [][0m
[0m[[31merror[0m] [0m	metadata.max.age.ms = 300000[0m
[0m[[31merror[0m] [0m	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor][0m
[0m[[31merror[0m] [0m	reconnect.backoff.ms = 50[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.window.factor = 0.8[0m
[0m[[31merror[0m] [0m	max.partition.fetch.bytes = 1048576[0m
[0m[[31merror[0m] [0m	bootstrap.servers = [localhost:18902][0m
[0m[[31merror[0m] [0m	ssl.keystore.type = JKS[0m
[0m[[31merror[0m] [0m	enable.auto.commit = false[0m
[0m[[31merror[0m] [0m	sasl.mechanism = GSSAPI[0m
[0m[[31merror[0m] [0m	interceptor.classes = null[0m
[0m[[31merror[0m] [0m	exclude.internal.topics = true[0m
[0m[[31merror[0m] [0m	ssl.truststore.password = null[0m
[0m[[31merror[0m] [0m	client.id = [0m
[0m[[31merror[0m] [0m	ssl.endpoint.identification.algorithm = null[0m
[0m[[31merror[0m] [0m	max.poll.records = 2147483647[0m
[0m[[31merror[0m] [0m	check.crcs = true[0m
[0m[[31merror[0m] [0m	request.timeout.ms = 40000[0m
[0m[[31merror[0m] [0m	heartbeat.interval.ms = 3000[0m
[0m[[31merror[0m] [0m	auto.commit.interval.ms = 5000[0m
[0m[[31merror[0m] [0m	receive.buffer.bytes = 65536[0m
[0m[[31merror[0m] [0m	ssl.truststore.type = JKS[0m
[0m[[31merror[0m] [0m	ssl.truststore.location = null[0m
[0m[[31merror[0m] [0m	ssl.keystore.password = null[0m
[0m[[31merror[0m] [0m	fetch.min.bytes = 1[0m
[0m[[31merror[0m] [0m	send.buffer.bytes = 131072[0m
[0m[[31merror[0m] [0m	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	group.id = spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-executor[0m
[0m[[31merror[0m] [0m	retry.backoff.ms = 100[0m
[0m[[31merror[0m] [0m	sasl.kerberos.kinit.cmd = /usr/bin/kinit[0m
[0m[[31merror[0m] [0m	sasl.kerberos.service.name = null[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.jitter = 0.05[0m
[0m[[31merror[0m] [0m	ssl.trustmanager.algorithm = PKIX[0m
[0m[[31merror[0m] [0m	ssl.key.password = null[0m
[0m[[31merror[0m] [0m	fetch.max.wait.ms = 500[0m
[0m[[31merror[0m] [0m	sasl.kerberos.min.time.before.relogin = 60000[0m
[0m[[31merror[0m] [0m	connections.max.idle.ms = 540000[0m
[0m[[31merror[0m] [0m	session.timeout.ms = 30000[0m
[0m[[31merror[0m] [0m	metrics.num.samples = 2[0m
[0m[[31merror[0m] [0m	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	ssl.protocol = TLS[0m
[0m[[31merror[0m] [0m	ssl.provider = null[0m
[0m[[31merror[0m] [0m	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1][0m
[0m[[31merror[0m] [0m	ssl.keystore.location = null[0m
[0m[[31merror[0m] [0m	ssl.cipher.suites = null[0m
[0m[[31merror[0m] [0m	security.protocol = PLAINTEXT[0m
[0m[[31merror[0m] [0m	ssl.keymanager.algorithm = SunX509[0m
[0m[[31merror[0m] [0m	metrics.sample.window.ms = 30000[0m
[0m[[31merror[0m] [0m	auto.offset.reset = none[0m
[0m[[31merror[0m] [0m[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO ConsumerConfig: ConsumerConfig values: [0m
[0m[[31merror[0m] [0m	metric.reporters = [][0m
[0m[[31merror[0m] [0m	metadata.max.age.ms = 300000[0m
[0m[[31merror[0m] [0m	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor][0m
[0m[[31merror[0m] [0m	reconnect.backoff.ms = 50[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.window.factor = 0.8[0m
[0m[[31merror[0m] [0m	max.partition.fetch.bytes = 1048576[0m
[0m[[31merror[0m] [0m	bootstrap.servers = [localhost:18902][0m
[0m[[31merror[0m] [0m	ssl.keystore.type = JKS[0m
[0m[[31merror[0m] [0m	enable.auto.commit = false[0m
[0m[[31merror[0m] [0m	sasl.mechanism = GSSAPI[0m
[0m[[31merror[0m] [0m	interceptor.classes = null[0m
[0m[[31merror[0m] [0m	exclude.internal.topics = true[0m
[0m[[31merror[0m] [0m	ssl.truststore.password = null[0m
[0m[[31merror[0m] [0m	client.id = consumer-5[0m
[0m[[31merror[0m] [0m	ssl.endpoint.identification.algorithm = null[0m
[0m[[31merror[0m] [0m	max.poll.records = 2147483647[0m
[0m[[31merror[0m] [0m	check.crcs = true[0m
[0m[[31merror[0m] [0m	request.timeout.ms = 40000[0m
[0m[[31merror[0m] [0m	heartbeat.interval.ms = 3000[0m
[0m[[31merror[0m] [0m	auto.commit.interval.ms = 5000[0m
[0m[[31merror[0m] [0m	receive.buffer.bytes = 65536[0m
[0m[[31merror[0m] [0m	ssl.truststore.type = JKS[0m
[0m[[31merror[0m] [0m	ssl.truststore.location = null[0m
[0m[[31merror[0m] [0m	ssl.keystore.password = null[0m
[0m[[31merror[0m] [0m	fetch.min.bytes = 1[0m
[0m[[31merror[0m] [0m	send.buffer.bytes = 131072[0m
[0m[[31merror[0m] [0m	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	group.id = spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-executor[0m
[0m[[31merror[0m] [0m	retry.backoff.ms = 100[0m
[0m[[31merror[0m] [0m	sasl.kerberos.kinit.cmd = /usr/bin/kinit[0m
[0m[[31merror[0m] [0m	sasl.kerberos.service.name = null[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.jitter = 0.05[0m
[0m[[31merror[0m] [0m	ssl.trustmanager.algorithm = PKIX[0m
[0m[[31merror[0m] [0m	ssl.key.password = null[0m
[0m[[31merror[0m] [0m	fetch.max.wait.ms = 500[0m
[0m[[31merror[0m] [0m	sasl.kerberos.min.time.before.relogin = 60000[0m
[0m[[31merror[0m] [0m	connections.max.idle.ms = 540000[0m
[0m[[31merror[0m] [0m	session.timeout.ms = 30000[0m
[0m[[31merror[0m] [0m	metrics.num.samples = 2[0m
[0m[[31merror[0m] [0m	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	ssl.protocol = TLS[0m
[0m[[31merror[0m] [0m	ssl.provider = null[0m
[0m[[31merror[0m] [0m	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1][0m
[0m[[31merror[0m] [0m	ssl.keystore.location = null[0m
[0m[[31merror[0m] [0m	ssl.cipher.suites = null[0m
[0m[[31merror[0m] [0m	security.protocol = PLAINTEXT[0m
[0m[[31merror[0m] [0m	ssl.keymanager.algorithm = SunX509[0m
[0m[[31merror[0m] [0m	metrics.sample.window.ms = 30000[0m
[0m[[31merror[0m] [0m	auto.offset.reset = none[0m
[0m[[31merror[0m] [0m[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO AppInfoParser: Kafka version : 0.10.0.1[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO AppInfoParser: Kafka commitId : a7a17cdec9eaa6c5[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO ConsumerConfig: ConsumerConfig values: [0m
[0m[[31merror[0m] [0m	metric.reporters = [][0m
[0m[[31merror[0m] [0m	metadata.max.age.ms = 300000[0m
[0m[[31merror[0m] [0m	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor][0m
[0m[[31merror[0m] [0m	reconnect.backoff.ms = 50[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.window.factor = 0.8[0m
[0m[[31merror[0m] [0m	max.partition.fetch.bytes = 1048576[0m
[0m[[31merror[0m] [0m	bootstrap.servers = [localhost:18902][0m
[0m[[31merror[0m] [0m	ssl.keystore.type = JKS[0m
[0m[[31merror[0m] [0m	enable.auto.commit = false[0m
[0m[[31merror[0m] [0m	sasl.mechanism = GSSAPI[0m
[0m[[31merror[0m] [0m	interceptor.classes = null[0m
[0m[[31merror[0m] [0m	exclude.internal.topics = true[0m
[0m[[31merror[0m] [0m	ssl.truststore.password = null[0m
[0m[[31merror[0m] [0m	client.id = [0m
[0m[[31merror[0m] [0m	ssl.endpoint.identification.algorithm = null[0m
[0m[[31merror[0m] [0m	max.poll.records = 2147483647[0m
[0m[[31merror[0m] [0m	check.crcs = true[0m
[0m[[31merror[0m] [0m	request.timeout.ms = 40000[0m
[0m[[31merror[0m] [0m	heartbeat.interval.ms = 3000[0m
[0m[[31merror[0m] [0m	auto.commit.interval.ms = 5000[0m
[0m[[31merror[0m] [0m	receive.buffer.bytes = 65536[0m
[0m[[31merror[0m] [0m	ssl.truststore.type = JKS[0m
[0m[[31merror[0m] [0m	ssl.truststore.location = null[0m
[0m[[31merror[0m] [0m	ssl.keystore.password = null[0m
[0m[[31merror[0m] [0m	fetch.min.bytes = 1[0m
[0m[[31merror[0m] [0m	send.buffer.bytes = 131072[0m
[0m[[31merror[0m] [0m	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	group.id = spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-executor[0m
[0m[[31merror[0m] [0m	retry.backoff.ms = 100[0m
[0m[[31merror[0m] [0m	sasl.kerberos.kinit.cmd = /usr/bin/kinit[0m
[0m[[31merror[0m] [0m	sasl.kerberos.service.name = null[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.jitter = 0.05[0m
[0m[[31merror[0m] [0m	ssl.trustmanager.algorithm = PKIX[0m
[0m[[31merror[0m] [0m	ssl.key.password = null[0m
[0m[[31merror[0m] [0m	fetch.max.wait.ms = 500[0m
[0m[[31merror[0m] [0m	sasl.kerberos.min.time.before.relogin = 60000[0m
[0m[[31merror[0m] [0m	connections.max.idle.ms = 540000[0m
[0m[[31merror[0m] [0m	session.timeout.ms = 30000[0m
[0m[[31merror[0m] [0m	metrics.num.samples = 2[0m
[0m[[31merror[0m] [0m	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	ssl.protocol = TLS[0m
[0m[[31merror[0m] [0m	ssl.provider = null[0m
[0m[[31merror[0m] [0m	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1][0m
[0m[[31merror[0m] [0m	ssl.keystore.location = null[0m
[0m[[31merror[0m] [0m	ssl.cipher.suites = null[0m
[0m[[31merror[0m] [0m	security.protocol = PLAINTEXT[0m
[0m[[31merror[0m] [0m	ssl.keymanager.algorithm = SunX509[0m
[0m[[31merror[0m] [0m	metrics.sample.window.ms = 30000[0m
[0m[[31merror[0m] [0m	auto.offset.reset = none[0m
[0m[[31merror[0m] [0m[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO ConsumerConfig: ConsumerConfig values: [0m
[0m[[31merror[0m] [0m	metric.reporters = [][0m
[0m[[31merror[0m] [0m	metadata.max.age.ms = 300000[0m
[0m[[31merror[0m] [0m	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor][0m
[0m[[31merror[0m] [0m	reconnect.backoff.ms = 50[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.window.factor = 0.8[0m
[0m[[31merror[0m] [0m	max.partition.fetch.bytes = 1048576[0m
[0m[[31merror[0m] [0m	bootstrap.servers = [localhost:18902][0m
[0m[[31merror[0m] [0m	ssl.keystore.type = JKS[0m
[0m[[31merror[0m] [0m	enable.auto.commit = false[0m
[0m[[31merror[0m] [0m	sasl.mechanism = GSSAPI[0m
[0m[[31merror[0m] [0m	interceptor.classes = null[0m
[0m[[31merror[0m] [0m	exclude.internal.topics = true[0m
[0m[[31merror[0m] [0m	ssl.truststore.password = null[0m
[0m[[31merror[0m] [0m	client.id = consumer-6[0m
[0m[[31merror[0m] [0m	ssl.endpoint.identification.algorithm = null[0m
[0m[[31merror[0m] [0m	max.poll.records = 2147483647[0m
[0m[[31merror[0m] [0m	check.crcs = true[0m
[0m[[31merror[0m] [0m	request.timeout.ms = 40000[0m
[0m[[31merror[0m] [0m	heartbeat.interval.ms = 3000[0m
[0m[[31merror[0m] [0m	auto.commit.interval.ms = 5000[0m
[0m[[31merror[0m] [0m	receive.buffer.bytes = 65536[0m
[0m[[31merror[0m] [0m	ssl.truststore.type = JKS[0m
[0m[[31merror[0m] [0m	ssl.truststore.location = null[0m
[0m[[31merror[0m] [0m	ssl.keystore.password = null[0m
[0m[[31merror[0m] [0m	fetch.min.bytes = 1[0m
[0m[[31merror[0m] [0m	send.buffer.bytes = 131072[0m
[0m[[31merror[0m] [0m	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	group.id = spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-executor[0m
[0m[[31merror[0m] [0m	retry.backoff.ms = 100[0m
[0m[[31merror[0m] [0m	sasl.kerberos.kinit.cmd = /usr/bin/kinit[0m
[0m[[31merror[0m] [0m	sasl.kerberos.service.name = null[0m
[0m[[31merror[0m] [0m	sasl.kerberos.ticket.renew.jitter = 0.05[0m
[0m[[31merror[0m] [0m	ssl.trustmanager.algorithm = PKIX[0m
[0m[[31merror[0m] [0m	ssl.key.password = null[0m
[0m[[31merror[0m] [0m	fetch.max.wait.ms = 500[0m
[0m[[31merror[0m] [0m	sasl.kerberos.min.time.before.relogin = 60000[0m
[0m[[31merror[0m] [0m	connections.max.idle.ms = 540000[0m
[0m[[31merror[0m] [0m	session.timeout.ms = 30000[0m
[0m[[31merror[0m] [0m	metrics.num.samples = 2[0m
[0m[[31merror[0m] [0m	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer[0m
[0m[[31merror[0m] [0m	ssl.protocol = TLS[0m
[0m[[31merror[0m] [0m	ssl.provider = null[0m
[0m[[31merror[0m] [0m	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1][0m
[0m[[31merror[0m] [0m	ssl.keystore.location = null[0m
[0m[[31merror[0m] [0m	ssl.cipher.suites = null[0m
[0m[[31merror[0m] [0m	security.protocol = PLAINTEXT[0m
[0m[[31merror[0m] [0m	ssl.keymanager.algorithm = SunX509[0m
[0m[[31merror[0m] [0m	metrics.sample.window.ms = 30000[0m
[0m[[31merror[0m] [0m	auto.offset.reset = none[0m
[0m[[31merror[0m] [0m[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO AppInfoParser: Kafka version : 0.10.0.1[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO AppInfoParser: Kafka commitId : a7a17cdec9eaa6c5[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO CodeGenerator: Code generated in 52.96893 ms[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO AbstractCoordinator: Discovered coordinator localhost:18902 (id: 2147483646 rack: null) for group spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-executor.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO AbstractCoordinator: Discovered coordinator localhost:18902 (id: 2147483646 rack: null) for group spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-executor.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO AbstractCoordinator: Discovered coordinator localhost:18902 (id: 2147483646 rack: null) for group spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-executor.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:56 INFO AbstractCoordinator: Discovered coordinator localhost:18902 (id: 2147483646 rack: null) for group spark-kafka-source-ec11464e-35f6-48e4-bef3-054f50de2726--1219373929-executor.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO CodeGenerator: Code generated in 59.599867 ms[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DataWritingSparkTask: Writer for partition 0 is committing.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DataWritingSparkTask: Writer for partition 0 committed.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2288 bytes result sent to driver[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DataWritingSparkTask: Writer for partition 3 is committing.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DataWritingSparkTask: Writer for partition 3 committed.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2245 bytes result sent to driver[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DataWritingSparkTask: Writer for partition 2 is committing.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DataWritingSparkTask: Writer for partition 2 committed.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 2245 bytes result sent to driver[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DataWritingSparkTask: Writer for partition 1 is committing.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DataWritingSparkTask: Writer for partition 1 committed.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2291 bytes result sent to driver[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 924 ms on localhost (executor driver) (1/4)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 896 ms on localhost (executor driver) (2/4)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 896 ms on localhost (executor driver) (3/4)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 897 ms on localhost (executor driver) (4/4)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool [0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DAGScheduler: ResultStage 0 (start at Simple.scala:49) finished in 1.369 s[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DAGScheduler: Job 0 finished: start at Simple.scala:49, took 1.502537 s[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO WriteToDataSourceV2Exec: Data source writer org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@61b64d4b is committing.[0m
[0m[[0minfo[0m] [0m-------------------------------------------[0m
[0m[[0minfo[0m] [0mBatch: 0[0m
[0m[[0minfo[0m] [0m-------------------------------------------[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO CodeGenerator: Code generated in 39.290432 ms[0m
[0m[[0minfo[0m] [0m+--------+-----------+[0m
[0m[[0minfo[0m] [0m|     key|      value|[0m
[0m[[0minfo[0m] [0m+--------+-----------+[0m
[0m[[0minfo[0m] [0m|[1]key_4|[1]string_4|[0m
[0m[[0minfo[0m] [0m|[1]key_2|[1]string_2|[0m
[0m[[0minfo[0m] [0m|[1]key_5|[1]string_5|[0m
[0m[[0minfo[0m] [0m|[1]key_3|[1]string_3|[0m
[0m[[0minfo[0m] [0m|[1]key_1|[1]string_1|[0m
[0m[[0minfo[0m] [0m+--------+-----------+[0m
[0m[[0minfo[0m] [0m[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO WriteToDataSourceV2Exec: Data source writer org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@61b64d4b committed.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO SparkContext: Starting job: start at Simple.scala:49[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DAGScheduler: Job 1 finished: start at Simple.scala:49, took 0.000035 s[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO MicroBatchExecution: Streaming query made progress: {[0m
[0m[[31merror[0m] [0m  "id" : "ffaedce5-66f3-429a-876a-bdbac34152a6",[0m
[0m[[31merror[0m] [0m  "runId" : "18f53bd6-d6a8-4f2a-97f6-cf6c0cb17ac1",[0m
[0m[[31merror[0m] [0m  "name" : null,[0m
[0m[[31merror[0m] [0m  "timestamp" : "2018-03-22T09:01:51.009Z",[0m
[0m[[31merror[0m] [0m  "batchId" : 0,[0m
[0m[[31merror[0m] [0m  "numInputRows" : 5,[0m
[0m[[31merror[0m] [0m  "processedRowsPerSecond" : 0.7692307692307693,[0m
[0m[[31merror[0m] [0m  "durationMs" : {[0m
[0m[[31merror[0m] [0m    "addBatch" : 2745,[0m
[0m[[31merror[0m] [0m    "getBatch" : 431,[0m
[0m[[31merror[0m] [0m    "getOffset" : 2839,[0m
[0m[[31merror[0m] [0m    "queryPlanning" : 331,[0m
[0m[[31merror[0m] [0m    "triggerExecution" : 6499,[0m
[0m[[31merror[0m] [0m    "walCommit" : 123[0m
[0m[[31merror[0m] [0m  },[0m
[0m[[31merror[0m] [0m  "stateOperators" : [ ],[0m
[0m[[31merror[0m] [0m  "sources" : [ {[0m
[0m[[31merror[0m] [0m    "description" : "KafkaSource[Subscribe[foo]]",[0m
[0m[[31merror[0m] [0m    "startOffset" : null,[0m
[0m[[31merror[0m] [0m    "endOffset" : {[0m
[0m[[31merror[0m] [0m      "foo" : {[0m
[0m[[31merror[0m] [0m        "2" : 1,[0m
[0m[[31merror[0m] [0m        "1" : 2,[0m
[0m[[31merror[0m] [0m        "3" : 1,[0m
[0m[[31merror[0m] [0m        "0" : 1[0m
[0m[[31merror[0m] [0m      }[0m
[0m[[31merror[0m] [0m    },[0m
[0m[[31merror[0m] [0m    "numInputRows" : 5,[0m
[0m[[31merror[0m] [0m    "processedRowsPerSecond" : 0.7692307692307693[0m
[0m[[31merror[0m] [0m  } ],[0m
[0m[[31merror[0m] [0m  "sink" : {[0m
[0m[[31merror[0m] [0m    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSinkProvider@3179aa23"[0m
[0m[[31merror[0m] [0m  }[0m
[0m[[31merror[0m] [0m}[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1521709317765,Map(spark.sql.shuffle.partitions -> 200, spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider))[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO KafkaSource: GetBatch called with start = Some({"foo":{"2":1,"1":2,"3":1,"0":1}}), end = {"foo":{"2":3,"1":3,"3":2,"0":2}}[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO KafkaSource: Partitions added: Map()[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO KafkaSource: GetBatch generating RDD of offset range: KafkaSourceRDDOffsetRange(foo-0,1,2,None), KafkaSourceRDDOffsetRange(foo-1,2,3,None), KafkaSourceRDDOffsetRange(foo-2,1,3,None), KafkaSourceRDDOffsetRange(foo-3,1,2,None)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO WriteToDataSourceV2Exec: Start processing data source writer: org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@e3d380d. The input RDD has 4 partitions.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO SparkContext: Starting job: start at Simple.scala:49[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DAGScheduler: Got job 2 (start at Simple.scala:49) with 4 output partitions[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DAGScheduler: Final stage: ResultStage 1 (start at Simple.scala:49)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DAGScheduler: Parents of final stage: List()[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DAGScheduler: Missing parents: List()[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at start at Simple.scala:49), which has no missing parents[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.8 KB, free 954.5 MB)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.1 KB, free 954.5 MB)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:38237 (size: 5.1 KB, free: 954.5 MB)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1039[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at Simple.scala:49) (first 15 tasks are for partitions Vector(0, 1, 2, 3))[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 8026 bytes)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, localhost, executor driver, partition 1, PROCESS_LOCAL, 8026 bytes)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, localhost, executor driver, partition 2, PROCESS_LOCAL, 8026 bytes)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, localhost, executor driver, partition 3, PROCESS_LOCAL, 8026 bytes)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO Executor: Running task 0.0 in stage 1.0 (TID 4)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DataWritingSparkTask: Writer for partition 0 is committing.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DataWritingSparkTask: Writer for partition 0 committed.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO Executor: Finished task 0.0 in stage 1.0 (TID 4). 2245 bytes result sent to driver[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO Executor: Running task 1.0 in stage 1.0 (TID 5)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DataWritingSparkTask: Writer for partition 1 is committing.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DataWritingSparkTask: Writer for partition 1 committed.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO Executor: Finished task 1.0 in stage 1.0 (TID 5). 2245 bytes result sent to driver[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO Executor: Running task 2.0 in stage 1.0 (TID 6)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DataWritingSparkTask: Writer for partition 2 is committing.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DataWritingSparkTask: Writer for partition 2 committed.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO Executor: Finished task 2.0 in stage 1.0 (TID 6). 2245 bytes result sent to driver[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO Executor: Running task 3.0 in stage 1.0 (TID 7)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DataWritingSparkTask: Writer for partition 3 is committing.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO DataWritingSparkTask: Writer for partition 3 committed.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO Executor: Finished task 3.0 in stage 1.0 (TID 7). 2291 bytes result sent to driver[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 37 ms on localhost (executor driver) (1/4)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 36 ms on localhost (executor driver) (2/4)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 37 ms on localhost (executor driver) (3/4)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 37 ms on localhost (executor driver) (4/4)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:57 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool [0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO DAGScheduler: ResultStage 1 (start at Simple.scala:49) finished in 0.078 s[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO DAGScheduler: Job 2 finished: start at Simple.scala:49, took 0.085255 s[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO WriteToDataSourceV2Exec: Data source writer org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@e3d380d is committing.[0m
[0m[[0minfo[0m] [0m-------------------------------------------[0m
[0m[[0minfo[0m] [0mBatch: 1[0m
[0m[[0minfo[0m] [0m-------------------------------------------[0m
[0m[[0minfo[0m] [0m+--------+-----------+[0m
[0m[[0minfo[0m] [0m|     key|      value|[0m
[0m[[0minfo[0m] [0m+--------+-----------+[0m
[0m[[0minfo[0m] [0m|[2]key_3|[2]string_3|[0m
[0m[[0minfo[0m] [0m|[2]key_4|[2]string_4|[0m
[0m[[0minfo[0m] [0m|[2]key_1|[2]string_1|[0m
[0m[[0minfo[0m] [0m|[2]key_2|[2]string_2|[0m
[0m[[0minfo[0m] [0m|[2]key_5|[2]string_5|[0m
[0m[[0minfo[0m] [0m+--------+-----------+[0m
[0m[[0minfo[0m] [0m[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO WriteToDataSourceV2Exec: Data source writer org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@e3d380d committed.[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO SparkContext: Starting job: start at Simple.scala:49[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO DAGScheduler: Job 3 finished: start at Simple.scala:49, took 0.000043 s[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO MicroBatchExecution: Streaming query made progress: {[0m
[0m[[31merror[0m] [0m  "id" : "ffaedce5-66f3-429a-876a-bdbac34152a6",[0m
[0m[[31merror[0m] [0m  "runId" : "18f53bd6-d6a8-4f2a-97f6-cf6c0cb17ac1",[0m
[0m[[31merror[0m] [0m  "name" : null,[0m
[0m[[31merror[0m] [0m  "timestamp" : "2018-03-22T09:01:57.751Z",[0m
[0m[[31merror[0m] [0m  "batchId" : 1,[0m
[0m[[31merror[0m] [0m  "numInputRows" : 5,[0m
[0m[[31merror[0m] [0m  "inputRowsPerSecond" : 0.7416196974191634,[0m
[0m[[31merror[0m] [0m  "processedRowsPerSecond" : 14.005602240896359,[0m
[0m[[31merror[0m] [0m  "durationMs" : {[0m
[0m[[31merror[0m] [0m    "addBatch" : 225,[0m
[0m[[31merror[0m] [0m    "getBatch" : 4,[0m
[0m[[31merror[0m] [0m    "getOffset" : 13,[0m
[0m[[31merror[0m] [0m    "queryPlanning" : 46,[0m
[0m[[31merror[0m] [0m    "triggerExecution" : 357,[0m
[0m[[31merror[0m] [0m    "walCommit" : 67[0m
[0m[[31merror[0m] [0m  },[0m
[0m[[31merror[0m] [0m  "stateOperators" : [ ],[0m
[0m[[31merror[0m] [0m  "sources" : [ {[0m
[0m[[31merror[0m] [0m    "description" : "KafkaSource[Subscribe[foo]]",[0m
[0m[[31merror[0m] [0m    "startOffset" : {[0m
[0m[[31merror[0m] [0m      "foo" : {[0m
[0m[[31merror[0m] [0m        "2" : 1,[0m
[0m[[31merror[0m] [0m        "1" : 2,[0m
[0m[[31merror[0m] [0m        "3" : 1,[0m
[0m[[31merror[0m] [0m        "0" : 1[0m
[0m[[31merror[0m] [0m      }[0m
[0m[[31merror[0m] [0m    },[0m
[0m[[31merror[0m] [0m    "endOffset" : {[0m
[0m[[31merror[0m] [0m      "foo" : {[0m
[0m[[31merror[0m] [0m        "2" : 3,[0m
[0m[[31merror[0m] [0m        "1" : 3,[0m
[0m[[31merror[0m] [0m        "3" : 2,[0m
[0m[[31merror[0m] [0m        "0" : 2[0m
[0m[[31merror[0m] [0m      }[0m
[0m[[31merror[0m] [0m    },[0m
[0m[[31merror[0m] [0m    "numInputRows" : 5,[0m
[0m[[31merror[0m] [0m    "inputRowsPerSecond" : 0.7416196974191634,[0m
[0m[[31merror[0m] [0m    "processedRowsPerSecond" : 14.005602240896359[0m
[0m[[31merror[0m] [0m  } ],[0m
[0m[[31merror[0m] [0m  "sink" : {[0m
[0m[[31merror[0m] [0m    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSinkProvider@3179aa23"[0m
[0m[[31merror[0m] [0m  }[0m
[0m[[31merror[0m] [0m}[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO MicroBatchExecution: Streaming query made progress: {[0m
[0m[[31merror[0m] [0m  "id" : "ffaedce5-66f3-429a-876a-bdbac34152a6",[0m
[0m[[31merror[0m] [0m  "runId" : "18f53bd6-d6a8-4f2a-97f6-cf6c0cb17ac1",[0m
[0m[[31merror[0m] [0m  "name" : null,[0m
[0m[[31merror[0m] [0m  "timestamp" : "2018-03-22T09:01:58.216Z",[0m
[0m[[31merror[0m] [0m  "batchId" : 2,[0m
[0m[[31merror[0m] [0m  "numInputRows" : 0,[0m
[0m[[31merror[0m] [0m  "inputRowsPerSecond" : 0.0,[0m
[0m[[31merror[0m] [0m  "processedRowsPerSecond" : 0.0,[0m
[0m[[31merror[0m] [0m  "durationMs" : {[0m
[0m[[31merror[0m] [0m    "getOffset" : 10,[0m
[0m[[31merror[0m] [0m    "triggerExecution" : 10[0m
[0m[[31merror[0m] [0m  },[0m
[0m[[31merror[0m] [0m  "stateOperators" : [ ],[0m
[0m[[31merror[0m] [0m  "sources" : [ {[0m
[0m[[31merror[0m] [0m    "description" : "KafkaSource[Subscribe[foo]]",[0m
[0m[[31merror[0m] [0m    "startOffset" : {[0m
[0m[[31merror[0m] [0m      "foo" : {[0m
[0m[[31merror[0m] [0m        "2" : 3,[0m
[0m[[31merror[0m] [0m        "1" : 3,[0m
[0m[[31merror[0m] [0m        "3" : 2,[0m
[0m[[31merror[0m] [0m        "0" : 2[0m
[0m[[31merror[0m] [0m      }[0m
[0m[[31merror[0m] [0m    },[0m
[0m[[31merror[0m] [0m    "endOffset" : {[0m
[0m[[31merror[0m] [0m      "foo" : {[0m
[0m[[31merror[0m] [0m        "2" : 3,[0m
[0m[[31merror[0m] [0m        "1" : 3,[0m
[0m[[31merror[0m] [0m        "3" : 2,[0m
[0m[[31merror[0m] [0m        "0" : 2[0m
[0m[[31merror[0m] [0m      }[0m
[0m[[31merror[0m] [0m    },[0m
[0m[[31merror[0m] [0m    "numInputRows" : 0,[0m
[0m[[31merror[0m] [0m    "inputRowsPerSecond" : 0.0,[0m
[0m[[31merror[0m] [0m    "processedRowsPerSecond" : 0.0[0m
[0m[[31merror[0m] [0m  } ],[0m
[0m[[31merror[0m] [0m  "sink" : {[0m
[0m[[31merror[0m] [0m    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSinkProvider@3179aa23"[0m
[0m[[31merror[0m] [0m  }[0m
[0m[[31merror[0m] [0m}[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 5[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 15[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 36[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 51[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 38[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 17[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 41[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 44[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 37[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 25[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 22[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 27[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 42[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 13[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 6[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 11[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 32[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 48[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 55[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 33[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 19[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 8[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 40[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 20[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 7[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 34[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 4[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 43[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 45[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 26[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 47[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 14[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 23[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 31[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 52[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 10[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 3[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 24[0m
[0m[[31merror[0m] [0m18/03/22 14:31:58 INFO ContextCleaner: Cleaned accumulator 16[0m
[0m[[31merror[0m] [0m18/03/22 14:31:59 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:38237 in memory (size: 5.1 KB, free: 954.5 MB)[0m
[0m[[31merror[0m] [0m18/03/22 14:31:59 INFO ContextCleaner: Cleaned accumulator 39[0m
[0m[[31merror[0m] [0m18/03/22 14:31:59 INFO ContextCleaner: Cleaned accumulator 21[0m
[0m[[31merror[0m] [0m18/03/22 14:31:59 INFO ContextCleaner: Cleaned accumulator 12[0m
[0m[[31merror[0m] [0m18/03/22 14:31:59 INFO ContextCleaner: Cleaned accumulator 53[0m
[0m[[31merror[0m] [0m18/03/22 14:31:59 INFO ContextCleaner: Cleaned accumulator 2[0m
[0m[[31merror[0m] [0m18/03/22 14:31:59 INFO ContextCleaner: Cleaned accumulator 49[0m
[0m[[31merror[0m] [0m18/03/22 14:31:59 INFO ContextCleaner: Cleaned accumulator 50[0m
[0m[[31merror[0m] [0m18/03/22 14:31:59 INFO ContextCleaner: Cleaned accumulator 54[0m
[0m[[31merror[0m] [0m18/03/22 14:31:59 INFO ContextCleaner: Cleaned accumulator 18[0m
[0m[[31merror[0m] [0m18/03/22 14:31:59 INFO ContextCleaner: Cleaned accumulator 9[0m
[0m[[31merror[0m] [0m18/03/22 14:31:59 INFO ContextCleaner: Cleaned accumulator 30[0m
[0m[[31merror[0m] [0m18/03/22 14:31:59 INFO ContextCleaner: Cleaned accumulator 46[0m
[0m[[31merror[0m] [0m18/03/22 14:31:59 INFO ContextCleaner: Cleaned accumulator 35[0m
[0m[[31merror[0m] [0m18/03/22 14:31:59 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:38237 in memory (size: 5.1 KB, free: 954.5 MB)[0m
[0m[[0minfo[0m] [0m*** Stopping stream[0m
[0m[[31merror[0m] [0m18/03/22 14:32:00 INFO DAGScheduler: Asked to cancel job group 18f53bd6-d6a8-4f2a-97f6-cf6c0cb17ac1[0m
[0m[[31merror[0m] [0m18/03/22 14:32:00 INFO DAGScheduler: Asked to cancel job group 18f53bd6-d6a8-4f2a-97f6-cf6c0cb17ac1[0m
[0m[[31merror[0m] [0m18/03/22 14:32:00 INFO MicroBatchExecution: Query [id = ffaedce5-66f3-429a-876a-bdbac34152a6, runId = 18f53bd6-d6a8-4f2a-97f6-cf6c0cb17ac1] was stopped[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped![0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO MemoryStore: MemoryStore cleared[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO BlockManager: BlockManager stopped[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO BlockManagerMaster: BlockManagerMaster stopped[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped![0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO SparkContext: Successfully stopped SparkContext[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO KafkaServer: shutting down broker on 18902[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO KafkaServer: [Kafka Server 1], shutting down[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO KafkaServer: [Kafka Server 1], Starting controlled shutdown[0m
[0m[[0minfo[0m] [0m*** Streaming terminated[0m
[0m[[0minfo[0m] [0m*** Stopping Kafka[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO KafkaController: [Controller 1]: Shutting down broker 1[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO KafkaServer: [Kafka Server 1], Controlled shutdown succeeded[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO SocketServer: [Socket Server on Broker 1], Shutting down[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO SocketServer: [Socket Server on Broker 1], Shutdown completed[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO KafkaRequestHandlerPool: [Kafka Request Handler on Broker 1], shutting down[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO KafkaRequestHandlerPool: [Kafka Request Handler on Broker 1], shut down completely[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO ClientQuotaManager$ThrottledRequestReaper: [ThrottledRequestReaper-Produce], Shutting down[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO ClientQuotaManager$ThrottledRequestReaper: [ThrottledRequestReaper-Produce], Stopped [0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO ClientQuotaManager$ThrottledRequestReaper: [ThrottledRequestReaper-Produce], Shutdown completed[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO ClientQuotaManager$ThrottledRequestReaper: [ThrottledRequestReaper-Fetch], Shutting down[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO ClientQuotaManager$ThrottledRequestReaper: [ThrottledRequestReaper-Fetch], Stopped [0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO ClientQuotaManager$ThrottledRequestReaper: [ThrottledRequestReaper-Fetch], Shutdown completed[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO KafkaApis: [KafkaApi-1] Shutdown complete.[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO ReplicaManager: [Replica Manager on Broker 1]: Shutting down[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 1] shutting down[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 1] shutdown completed[0m
[0m[[31merror[0m] [0m18/03/22 14:32:01 INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-1], Shutting down[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-1], Stopped [0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-1], Shutdown completed[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-1], Shutting down[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-1], Stopped [0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-1], Shutdown completed[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO ReplicaManager: [Replica Manager on Broker 1]: Shut down completely[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO LogManager: Shutting down.[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO LogCleaner: Shutting down the log cleaner.[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO LogCleaner: [kafka-log-cleaner-thread-0], Shutting down[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO LogCleaner: [kafka-log-cleaner-thread-0], Stopped [0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO LogCleaner: [kafka-log-cleaner-thread-0], Shutdown completed[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO LogManager: Shutdown complete.[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO GroupCoordinator: [GroupCoordinator 1]: Shutting down.[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-1], Shutting down[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-1], Stopped [0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-1], Shutdown completed[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-1], Shutting down[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-1], Stopped [0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-1], Shutdown completed[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO GroupCoordinator: [GroupCoordinator 1]: Shutdown complete.[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO PartitionStateMachine: [Partition state machine on Controller 1]: Stopped partition state machine[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO ReplicaStateMachine: [Replica state machine on controller 1]: Stopped replica state machine[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO RequestSendThread: [Controller-1-to-broker-1-send-thread], Shutting down[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO RequestSendThread: [Controller-1-to-broker-1-send-thread], Stopped [0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO RequestSendThread: [Controller-1-to-broker-1-send-thread], Shutdown completed[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO KafkaController: [Controller 1]: Broker 1 resigned as the controller[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO ZkEventThread: Terminate ZkClient event thread.[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO PrepRequestProcessor: Processed session termination for sessionid: 0x1624cf118e20000[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO NIOServerCnxn: Closed socket connection for client /127.0.0.1:40776 which had sessionid 0x1624cf118e20000[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO ZooKeeper: Session: 0x1624cf118e20000 closed[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO ClientCnxn: EventThread shut down[0m
[0m[[31merror[0m] [0m18/03/22 14:32:02 INFO KafkaServer: [Kafka Server 1], shut down completed[0m
[0m[[31merror[0m] [0m18/03/22 14:32:07 INFO KafkaServer: shutting down zookeeper on 18901[0m
[0m[[31merror[0m] [0m18/03/22 14:32:07 INFO Zookeeper: shutting down Zookeeper on 18901[0m
[0m[[31merror[0m] [0m18/03/22 14:32:07 INFO NIOServerCnxn: Closed socket connection for client /127.0.0.1:40778 which had sessionid 0x1624cf118e20001[0m
[0m[[31merror[0m] [0m18/03/22 14:32:07 INFO NIOServerCnxnFactory: NIOServerCnxn factory exited run method[0m
[0m[[31merror[0m] [0m18/03/22 14:32:07 INFO ClientCnxn: Unable to read additional data from server sessionid 0x1624cf118e20001, likely server has closed socket, closing socket connection and attempting reconnect[0m
[0m[[31merror[0m] [0m18/03/22 14:32:07 INFO ZooKeeperServer: shutting down[0m
[0m[[31merror[0m] [0m18/03/22 14:32:07 INFO SessionTrackerImpl: Shutting down[0m
[0m[[31merror[0m] [0m18/03/22 14:32:07 INFO PrepRequestProcessor: Shutting down[0m
[0m[[31merror[0m] [0m18/03/22 14:32:07 INFO SyncRequestProcessor: Shutting down[0m
[0m[[31merror[0m] [0m18/03/22 14:32:07 INFO PrepRequestProcessor: PrepRequestProcessor exited loop![0m
[0m[[31merror[0m] [0m18/03/22 14:32:07 INFO SyncRequestProcessor: SyncRequestProcessor exited![0m
[0m[[0minfo[0m] [0m*** done[0m
[0m[[31merror[0m] [0m18/03/22 14:32:07 INFO FinalRequestProcessor: shutdown of request processor complete[0m
[0m[[31merror[0m] [0m18/03/22 14:32:07 INFO ZkClient: zookeeper state changed (Disconnected)[0m
[0m[[31merror[0m] [0m18/03/22 14:32:08 INFO SessionTrackerImpl: SessionTrackerImpl exited loop![0m
[0m[[31merror[0m] [0m18/03/22 14:32:08 INFO ShutdownHookManager: Shutdown hook called[0m
[0m[[31merror[0m] [0m18/03/22 14:32:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-56a6e513-fc3b-464b-9612-1c1f2d242fb6[0m
[0m[[31merror[0m] [0m18/03/22 14:32:08 INFO ShutdownHookManager: Deleting directory /tmp/temporaryReader-29983c59-dc90-4e9d-9581-a2398c923b7f[0m
[0m[[31merror[0m] [0m18/03/22 14:32:08 INFO ShutdownHookManager: Deleting directory /tmp/temporary-b940620c-4d38-444d-b100-c801731d67d8[0m
